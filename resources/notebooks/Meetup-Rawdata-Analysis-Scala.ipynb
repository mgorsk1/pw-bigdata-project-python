{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://repo1.maven.org/maven2/xmlenc/xmlenc/0.52/xmlenc-0.52-sources.jar\n",
      "Downloading https://repo1.maven.org/maven2/com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9-sources.jar\n",
      "Downloading https://repo1.maven.org/maven2/xmlenc/xmlenc/0.52/xmlenc-0.52-sources.jar.sha1\n",
      "Downloading https://repo1.maven.org/maven2/com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9-sources.jar.sha1\n",
      "Downloading https://repo1.maven.org/maven2/xerces/xercesImpl/2.9.1/xercesImpl-2.9.1-sources.jar\n",
      "Downloading https://repo1.maven.org/maven2/xerces/xercesImpl/2.9.1/xercesImpl-2.9.1-sources.jar.sha1\n",
      "Downloading https://repo1.maven.org/maven2/commons-beanutils/commons-beanutils-core/1.8.0/commons-beanutils-core-1.8.0-sources.jar\n",
      "Downloading https://repo1.maven.org/maven2/commons-beanutils/commons-beanutils-core/1.8.0/commons-beanutils-core-1.8.0-sources.jar.sha1\n",
      "Downloading https://repo1.maven.org/maven2/mx4j/mx4j/3.0.2/mx4j-3.0.2-sources.jar\n",
      "Downloading https://repo1.maven.org/maven2/mx4j/mx4j/3.0.2/mx4j-3.0.2-sources.jar.sha1\n",
      "Downloading https://repo1.maven.org/maven2/xmlenc/xmlenc/0.52/xmlenc-0.52-sources.jar\n",
      "Downloading https://repo1.maven.org/maven2/xmlenc/xmlenc/0.52/xmlenc-0.52-sources.jar.sha1\n",
      "Downloading https://repo1.maven.org/maven2/commons-beanutils/commons-beanutils-core/1.8.0/commons-beanutils-core-1.8.0-sources.jar\n",
      "Downloading https://repo1.maven.org/maven2/com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9-sources.jar.sha1\n",
      "Downloading https://repo1.maven.org/maven2/com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9-sources.jar\n",
      "Downloading https://repo1.maven.org/maven2/commons-beanutils/commons-beanutils-core/1.8.0/commons-beanutils-core-1.8.0-sources.jar.sha1\n",
      "Downloading https://repo1.maven.org/maven2/mx4j/mx4j/3.0.2/mx4j-3.0.2-sources.jar\n",
      "Downloading https://repo1.maven.org/maven2/mx4j/mx4j/3.0.2/mx4j-3.0.2-sources.jar.sha1\n",
      "Downloading https://repo1.maven.org/maven2/xerces/xercesImpl/2.9.1/xercesImpl-2.9.1-sources.jar\n",
      "Downloading https://repo1.maven.org/maven2/xerces/xercesImpl/2.9.1/xercesImpl-2.9.1-sources.jar.sha1\n",
      "Downloading https://repo1.maven.org/maven2/xerces/xercesImpl/2.9.1/xercesImpl-2.9.1-sources.jar.sha1\n",
      "Downloading https://repo1.maven.org/maven2/xerces/xercesImpl/2.9.1/xercesImpl-2.9.1-sources.jar\n",
      "Downloading https://repo1.maven.org/maven2/commons-beanutils/commons-beanutils-core/1.8.0/commons-beanutils-core-1.8.0-sources.jar\n",
      "Downloading https://repo1.maven.org/maven2/commons-beanutils/commons-beanutils-core/1.8.0/commons-beanutils-core-1.8.0-sources.jar.sha1\n",
      "Downloading https://repo1.maven.org/maven2/mx4j/mx4j/3.0.2/mx4j-3.0.2-sources.jar\n",
      "Downloading https://repo1.maven.org/maven2/mx4j/mx4j/3.0.2/mx4j-3.0.2-sources.jar.sha1\n",
      "Downloading https://repo1.maven.org/maven2/com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9-sources.jar\n",
      "Downloading https://repo1.maven.org/maven2/xmlenc/xmlenc/0.52/xmlenc-0.52-sources.jar\n",
      "Downloading https://repo1.maven.org/maven2/com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9-sources.jar.sha1\n",
      "Downloading https://repo1.maven.org/maven2/xmlenc/xmlenc/0.52/xmlenc-0.52-sources.jar.sha1\n"
     ]
    }
   ],
   "source": [
    "interp.load.ivy(\"org.apache.spark\"       %% \"spark-core\"                  % \"2.2.1\")\n",
    "interp.load.ivy(\"org.apache.spark\"       %% \"spark-sql\"                   % \"2.2.1\")\n",
    "interp.load.ivy(\"org.apache.spark\"       %% \"spark-streaming\"             % \"2.2.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-core/2.6.2/jackson-core-2.6.2.pom\n",
      "Downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-core/2.6.2/jackson-core-2.6.2.pom.sha1\n",
      "Downloaded https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-core/2.6.2/jackson-core-2.6.2.pom\n",
      "Downloaded https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-core/2.6.2/jackson-core-2.6.2.pom.sha1\n",
      "Downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-core/2.6.2/\n",
      "Downloaded https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-core/2.6.2/\n",
      "Downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-core/2.6.2/jackson-core-2.6.2.jar\n",
      "Downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-core/2.6.2/jackson-core-2.6.2.jar.sha1\n",
      "Downloaded https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-core/2.6.2/jackson-core-2.6.2.jar.sha1\n",
      "Downloaded https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-core/2.6.2/jackson-core-2.6.2.jar\n",
      "Downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-core/2.6.2/jackson-core-2.6.2-sources.jar\n",
      "Downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-core/2.6.2/jackson-core-2.6.2-sources.jar.sha1\n",
      "Downloaded https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-core/2.6.2/jackson-core-2.6.2-sources.jar.sha1\n",
      "Downloaded https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-core/2.6.2/jackson-core-2.6.2-sources.jar\n",
      "Downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-databind/2.6.2/jackson-databind-2.6.2.pom\n",
      "Downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-databind/2.6.2/jackson-databind-2.6.2.pom.sha1\n",
      "Downloaded https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-databind/2.6.2/jackson-databind-2.6.2.pom.sha1\n",
      "Downloaded https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-databind/2.6.2/jackson-databind-2.6.2.pom\n",
      "Downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-databind/2.6.2/\n",
      "Downloaded https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-databind/2.6.2/\n",
      "Downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-databind/2.6.2/jackson-databind-2.6.2.jar\n",
      "Downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-databind/2.6.2/jackson-databind-2.6.2.jar.sha1\n",
      "Downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-annotations/2.6.0/jackson-annotations-2.6.0.jar\n",
      "Downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-annotations/2.6.0/jackson-annotations-2.6.0.jar.sha1\n",
      "Downloaded https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-databind/2.6.2/jackson-databind-2.6.2.jar.sha1\n",
      "Downloaded https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-annotations/2.6.0/jackson-annotations-2.6.0.jar.sha1\n",
      "Downloaded https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-annotations/2.6.0/jackson-annotations-2.6.0.jar\n",
      "Downloaded https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-databind/2.6.2/jackson-databind-2.6.2.jar\n",
      "Downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-databind/2.6.2/jackson-databind-2.6.2-sources.jar.sha1\n",
      "Downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-annotations/2.6.0/jackson-annotations-2.6.0-sources.jar.sha1\n",
      "Downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-annotations/2.6.0/jackson-annotations-2.6.0-sources.jar\n",
      "Downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-databind/2.6.2/jackson-databind-2.6.2-sources.jar\n",
      "Downloaded https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-databind/2.6.2/jackson-databind-2.6.2-sources.jar.sha1\n",
      "Downloaded https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-annotations/2.6.0/jackson-annotations-2.6.0-sources.jar.sha1\n",
      "Downloaded https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-annotations/2.6.0/jackson-annotations-2.6.0-sources.jar\n",
      "Downloaded https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-databind/2.6.2/jackson-databind-2.6.2-sources.jar\n",
      "Downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-annotations/2.6.2/jackson-annotations-2.6.2.pom\n",
      "Downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-annotations/2.6.2/jackson-annotations-2.6.2.pom.sha1\n",
      "Downloaded https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-annotations/2.6.2/jackson-annotations-2.6.2.pom\n",
      "Downloaded https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-annotations/2.6.2/jackson-annotations-2.6.2.pom.sha1\n",
      "Downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-annotations/2.6.2/\n",
      "Downloaded https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-annotations/2.6.2/\n",
      "Downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-annotations/2.6.2/jackson-annotations-2.6.2.jar\n",
      "Downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-annotations/2.6.2/jackson-annotations-2.6.2.jar.sha1\n",
      "Downloaded https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-annotations/2.6.2/jackson-annotations-2.6.2.jar.sha1\n",
      "Downloaded https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-annotations/2.6.2/jackson-annotations-2.6.2.jar\n",
      "Downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-annotations/2.6.2/jackson-annotations-2.6.2-sources.jar.sha1\n",
      "Downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-annotations/2.6.2/jackson-annotations-2.6.2-sources.jar\n",
      "Downloaded https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-annotations/2.6.2/jackson-annotations-2.6.2-sources.jar.sha1\n",
      "Downloaded https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-annotations/2.6.2/jackson-annotations-2.6.2-sources.jar\n"
     ]
    }
   ],
   "source": [
    "interp.load.ivy(\"com.fasterxml.jackson.core\" % \"jackson-core\" % \"2.6.2\")\n",
    "interp.load.ivy(\"com.fasterxml.jackson.core\" % \"jackson-databind\" % \"2.6.2\")\n",
    "interp.load.ivy(\"com.fasterxml.jackson.core\" % \"jackson-annotations\" % \"2.6.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mjava.io.File\n",
       "\n",
       "// val d = new File(\"/home/jovyan/work/jars/\")\n",
       "\n",
       "// if (d.exists && d.isDirectory) {\n",
       "//     d.listFiles.filter(_.isFile).toList\n",
       "// } else {\n",
       "//     println(\"dupa\")\n",
       "// }\n",
       "\n",
       "// def load_jar(pathArg: String): Unit = {\n",
       "//     val path = java.nio.file.FileSystems.getDefault().getPath(pathArg);\n",
       "//     val x = ammonite.ops.Path(path);\n",
       "//     interp.load.cp(x);\n",
       "// }\n",
       "\n",
       "// for (e <- d) println(e)\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mpath\u001b[39m: \u001b[32mjava\u001b[39m.\u001b[32mnio\u001b[39m.\u001b[32mfile\u001b[39m.\u001b[32mPath\u001b[39m = /home/jovyan/work/jars/magellan-1.0.5-s_2.11.jar\n",
       "\u001b[36mx\u001b[39m: \u001b[32mops\u001b[39m.\u001b[32mPath\u001b[39m = root/\u001b[32m'home\u001b[39m/\u001b[32m'jovyan\u001b[39m/\u001b[32m'work\u001b[39m/\u001b[32m'jars\u001b[39m/\u001b[32m\"magellan-1.0.5-s_2.11.jar\"\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import java.io.File\n",
    "\n",
    "// val d = new File(\"/home/jovyan/work/jars/\")\n",
    "\n",
    "// if (d.exists && d.isDirectory) {\n",
    "//     d.listFiles.filter(_.isFile).toList\n",
    "// } else {\n",
    "//     println(\"dupa\")\n",
    "// }\n",
    "\n",
    "// def load_jar(pathArg: String): Unit = {\n",
    "//     val path = java.nio.file.FileSystems.getDefault().getPath(pathArg);\n",
    "//     val x = ammonite.ops.Path(path);\n",
    "//     interp.load.cp(x);\n",
    "// }\n",
    "\n",
    "// for (e <- d) println(e)\n",
    "\n",
    "val path = java.nio.file.FileSystems.getDefault().getPath(\"/home/jovyan/work/jars/magellan-1.0.5-s_2.11.jar\");\n",
    "val x = ammonite.ops.Path(path);\n",
    "interp.load.cp(x);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connect to Spark & Elasticsearch, gather raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "19/01/03 19:14:08 INFO SparkContext: Running Spark version 2.2.1\n",
      "19/01/03 19:14:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "19/01/03 19:14:09 INFO SparkContext: Submitted application: pw-bd-project-meetup-analytics\n",
      "19/01/03 19:14:09 INFO SecurityManager: Changing view acls to: jovyan\n",
      "19/01/03 19:14:09 INFO SecurityManager: Changing modify acls to: jovyan\n",
      "19/01/03 19:14:09 INFO SecurityManager: Changing view acls groups to: \n",
      "19/01/03 19:14:09 INFO SecurityManager: Changing modify acls groups to: \n",
      "19/01/03 19:14:09 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(jovyan); groups with view permissions: Set(); users  with modify permissions: Set(jovyan); groups with modify permissions: Set()\n",
      "19/01/03 19:14:10 INFO Utils: Successfully started service 'sparkDriver' on port 40385.\n",
      "19/01/03 19:14:10 INFO SparkEnv: Registering MapOutputTracker\n",
      "19/01/03 19:14:10 INFO SparkEnv: Registering BlockManagerMaster\n",
      "19/01/03 19:14:10 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "19/01/03 19:14:10 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "19/01/03 19:14:10 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-064a8808-de1f-4f05-a769-674fc293c63a\n",
      "19/01/03 19:14:10 INFO MemoryStore: MemoryStore started with capacity 612.6 MB\n",
      "19/01/03 19:14:10 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "19/01/03 19:14:10 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "19/01/03 19:14:10 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://171.17.238.3:4040\n",
      "19/01/03 19:14:10 INFO Executor: Starting executor ID driver on host localhost\n",
      "19/01/03 19:14:10 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34619.\n",
      "19/01/03 19:14:10 INFO NettyBlockTransferService: Server created on 171.17.238.3:34619\n",
      "19/01/03 19:14:10 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "19/01/03 19:14:10 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 171.17.238.3, 34619, None)\n",
      "19/01/03 19:14:10 INFO BlockManagerMasterEndpoint: Registering block manager 171.17.238.3:34619 with 612.6 MB RAM, BlockManagerId(driver, 171.17.238.3, 34619, None)\n",
      "19/01/03 19:14:10 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 171.17.238.3, 34619, None)\n",
      "19/01/03 19:14:10 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 171.17.238.3, 34619, None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.SparkContext\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.SparkSession\n",
       "\n",
       "// setup streaming\n",
       "\u001b[39m\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@17724e1f\n",
       "\u001b[36msc\u001b[39m: \u001b[32mSparkContext\u001b[39m = org.apache.spark.SparkContext@388b71f3\n",
       "\u001b[36msqc\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mSQLContext\u001b[39m = org.apache.spark.sql.SQLContext@1b25a9fc"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.SparkContext\n",
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "// setup streaming\n",
    "val spark = SparkSession\n",
    "  .builder\n",
    "  .appName(\"pw-bd-project-meetup-analytics\")\n",
    "  .master(\"local[*]\")\n",
    "  .getOrCreate()\n",
    "\n",
    "val sc = spark.sparkContext\n",
    "val sqc = spark.sqlContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.1\n"
     ]
    }
   ],
   "source": [
    "println(sc.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19/01/03 19:14:11 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/jovyan/work/spark-warehouse/').\n",
      "19/01/03 19:14:11 INFO SharedState: Warehouse path is 'file:/home/jovyan/work/spark-warehouse/'.\n",
      "19/01/03 19:14:11 WARN SharedState: URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory\n",
      "19/01/03 19:14:12 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint\n",
      "19/01/03 19:14:13 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 216.5 KB, free 612.4 MB)\n",
      "19/01/03 19:14:13 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.5 KB, free 612.4 MB)\n",
      "19/01/03 19:14:13 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 171.17.238.3:34619 (size: 20.5 KB, free: 612.6 MB)\n",
      "19/01/03 19:14:13 INFO SparkContext: Created broadcast 0 from newAPIHadoopFile at ShapefileRelation.scala:42\n",
      "19/01/03 19:14:13 INFO FileInputFormat: Total input paths to process : 1\n",
      "19/01/03 19:14:13 INFO SparkContext: Starting job: collectAsMap at ShapefileRelation.scala:58\n",
      "19/01/03 19:14:13 INFO DAGScheduler: Got job 0 (collectAsMap at ShapefileRelation.scala:58) with 1 output partitions\n",
      "19/01/03 19:14:13 INFO DAGScheduler: Final stage: ResultStage 0 (collectAsMap at ShapefileRelation.scala:58)\n",
      "19/01/03 19:14:13 INFO DAGScheduler: Parents of final stage: List()\n",
      "19/01/03 19:14:13 INFO DAGScheduler: Missing parents: List()\n",
      "19/01/03 19:14:14 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at map at ShapefileRelation.scala:47), which has no missing parents\n",
      "19/01/03 19:14:14 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 2.6 KB, free 612.4 MB)\n",
      "19/01/03 19:14:14 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 1621.0 B, free 612.4 MB)\n",
      "19/01/03 19:14:14 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 171.17.238.3:34619 (size: 1621.0 B, free: 612.6 MB)\n",
      "19/01/03 19:14:14 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1006\n",
      "19/01/03 19:14:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at ShapefileRelation.scala:47) (first 15 tasks are for partitions Vector(0))\n",
      "19/01/03 19:14:14 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks\n",
      "19/01/03 19:14:14 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4915 bytes)\n",
      "19/01/03 19:14:14 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "19/01/03 19:14:14 INFO NewHadoopRDD: Input split: file:/home/jovyan/work/data/timezones/new/tz_world.shx:0+222036\n",
      "19/01/03 19:14:14 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 940 bytes result sent to driver\n",
      "19/01/03 19:14:14 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 292 ms on localhost (executor driver) (1/1)\n",
      "19/01/03 19:14:14 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "19/01/03 19:14:14 INFO DAGScheduler: ResultStage 0 (collectAsMap at ShapefileRelation.scala:58) finished in 0.324 s\n",
      "19/01/03 19:14:14 INFO DAGScheduler: Job 0 finished: collectAsMap at ShapefileRelation.scala:58, took 0.551693 s\n",
      "19/01/03 19:14:14 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 216.5 KB, free 612.2 MB)\n",
      "19/01/03 19:14:14 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 20.5 KB, free 612.1 MB)\n",
      "19/01/03 19:14:14 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 171.17.238.3:34619 (size: 20.5 KB, free: 612.6 MB)\n",
      "19/01/03 19:14:14 INFO SparkContext: Created broadcast 2 from newAPIHadoopFile at ShapefileRelation.scala:62\n",
      "19/01/03 19:14:14 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 216.5 KB, free 611.9 MB)\n",
      "19/01/03 19:14:14 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 20.5 KB, free 611.9 MB)\n",
      "19/01/03 19:14:14 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 171.17.238.3:34619 (size: 20.5 KB, free: 612.5 MB)\n",
      "19/01/03 19:14:14 INFO SparkContext: Created broadcast 3 from newAPIHadoopFile at ShapefileRelation.scala:69\n",
      "19/01/03 19:14:14 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 171.17.238.3:34619 in memory (size: 20.5 KB, free: 612.6 MB)\n",
      "19/01/03 19:14:14 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 171.17.238.3:34619 in memory (size: 1621.0 B, free: 612.6 MB)\n",
      "19/01/03 19:14:14 INFO FileInputFormat: Total input paths to process : 1\n",
      "19/01/03 19:14:14 INFO FileInputFormat: Total input paths to process : 1\n",
      "19/01/03 19:14:15 INFO CodeGenerator: Code generated in 314.1395 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mmagellan._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mtimezoneData\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mpackage\u001b[39m.\u001b[32mDataFrame\u001b[39m = [point: point, polyline: polyline ... 3 more fields]\n",
       "\u001b[36mres5_2\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mpackage\u001b[39m.\u001b[32mDataFrame\u001b[39m = [point: point, polyline: polyline ... 3 more fields]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import magellan._\n",
    "\n",
    "val timezoneData = sqc\n",
    "    .read\n",
    "    .format(\"magellan\")\n",
    "    .load(\"/home/jovyan/work/data/timezones/new/\")\n",
    "\n",
    "timezoneData.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19/01/03 19:14:16 INFO ContextCleaner: Cleaned accumulator 51\n",
      "19/01/03 19:14:16 INFO CodeGenerator: Code generated in 20.1366 ms\n",
      "19/01/03 19:14:16 INFO CodeGenerator: Code generated in 9.2453 ms\n",
      "19/01/03 19:14:16 INFO SparkContext: Starting job: count at cmd6.sc:1\n",
      "19/01/03 19:14:16 INFO DAGScheduler: Registering RDD 4 (map at ShapefileRelation.scala:76)\n",
      "19/01/03 19:14:16 INFO DAGScheduler: Registering RDD 5 (map at ShapefileRelation.scala:80)\n",
      "19/01/03 19:14:16 INFO DAGScheduler: Registering RDD 16 (count at cmd6.sc:1)\n",
      "19/01/03 19:14:16 INFO DAGScheduler: Got job 1 (count at cmd6.sc:1) with 1 output partitions\n",
      "19/01/03 19:14:16 INFO DAGScheduler: Final stage: ResultStage 4 (count at cmd6.sc:1)\n",
      "19/01/03 19:14:16 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)\n",
      "19/01/03 19:14:16 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 3)\n",
      "19/01/03 19:14:16 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[4] at map at ShapefileRelation.scala:76), which has no missing parents\n",
      "19/01/03 19:14:16 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 3.3 KB, free 612.1 MB)\n",
      "19/01/03 19:14:16 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 2023.0 B, free 612.1 MB)\n",
      "19/01/03 19:14:16 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 171.17.238.3:34619 (size: 2023.0 B, free: 612.6 MB)\n",
      "19/01/03 19:14:16 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1006\n",
      "19/01/03 19:14:16 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[4] at map at ShapefileRelation.scala:76) (first 15 tasks are for partitions Vector(0))\n",
      "19/01/03 19:14:16 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks\n",
      "19/01/03 19:14:16 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[5] at map at ShapefileRelation.scala:80), which has no missing parents\n",
      "19/01/03 19:14:16 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 4904 bytes)\n",
      "19/01/03 19:14:16 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
      "19/01/03 19:14:16 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 3.3 KB, free 612.1 MB)\n",
      "19/01/03 19:14:16 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 2023.0 B, free 612.1 MB)\n",
      "19/01/03 19:14:16 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 171.17.238.3:34619 (size: 2023.0 B, free: 612.6 MB)\n",
      "19/01/03 19:14:16 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1006\n",
      "19/01/03 19:14:16 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[5] at map at ShapefileRelation.scala:80) (first 15 tasks are for partitions Vector(0))\n",
      "19/01/03 19:14:16 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks\n",
      "19/01/03 19:14:16 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 4904 bytes)\n",
      "19/01/03 19:14:16 INFO NewHadoopRDD: Input split: file:/home/jovyan/work/data/timezones/new/tz_world.shp:100+37221656\n",
      "19/01/03 19:14:16 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)\n",
      "19/01/03 19:14:16 INFO NewHadoopRDD: Input split: file:/home/jovyan/work/data/timezones/new/tz_world.dbf:0+860067\n",
      "19/01/03 19:14:17 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1024 bytes result sent to driver\n",
      "19/01/03 19:14:17 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 806 ms on localhost (executor driver) (1/1)\n",
      "19/01/03 19:14:17 INFO DAGScheduler: ShuffleMapStage 2 (map at ShapefileRelation.scala:80) finished in 0.808 s\n",
      "19/01/03 19:14:17 INFO DAGScheduler: looking for newly runnable stages\n",
      "19/01/03 19:14:17 INFO DAGScheduler: running: Set(ShuffleMapStage 1)\n",
      "19/01/03 19:14:17 INFO DAGScheduler: waiting: Set(ShuffleMapStage 3, ResultStage 4)\n",
      "19/01/03 19:14:17 INFO DAGScheduler: failed: Set()\n",
      "19/01/03 19:14:17 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "19/01/03 19:14:17 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 171.17.238.3:34619 in memory (size: 2023.0 B, free: 612.6 MB)\n",
      "19/01/03 19:14:18 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1024 bytes result sent to driver\n",
      "19/01/03 19:14:18 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1858 ms on localhost (executor driver) (1/1)\n",
      "19/01/03 19:14:18 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "19/01/03 19:14:18 INFO DAGScheduler: ShuffleMapStage 1 (map at ShapefileRelation.scala:76) finished in 1.860 s\n",
      "19/01/03 19:14:18 INFO DAGScheduler: looking for newly runnable stages\n",
      "19/01/03 19:14:18 INFO DAGScheduler: running: Set()\n",
      "19/01/03 19:14:18 INFO DAGScheduler: waiting: Set(ShuffleMapStage 3, ResultStage 4)\n",
      "19/01/03 19:14:18 INFO DAGScheduler: failed: Set()\n",
      "19/01/03 19:14:18 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[16] at count at cmd6.sc:1), which has no missing parents\n",
      "19/01/03 19:14:18 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 37.7 KB, free 612.1 MB)\n",
      "19/01/03 19:14:18 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 13.6 KB, free 612.1 MB)\n",
      "19/01/03 19:14:18 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 171.17.238.3:34619 (size: 13.6 KB, free: 612.5 MB)\n",
      "19/01/03 19:14:18 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1006\n",
      "19/01/03 19:14:18 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[16] at count at cmd6.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "19/01/03 19:14:18 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks\n",
      "19/01/03 19:14:18 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 4673 bytes)\n",
      "19/01/03 19:14:18 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)\n",
      "19/01/03 19:14:18 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks\n",
      "19/01/03 19:14:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms\n",
      "19/01/03 19:14:18 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks\n",
      "19/01/03 19:14:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "19/01/03 19:14:18 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 171.17.238.3:34619 in memory (size: 2023.0 B, free: 612.5 MB)\n",
      "19/01/03 19:14:20 INFO MemoryStore: Block rdd_13_0 stored as values in memory (estimated size 40.8 MB, free 571.3 MB)\n",
      "19/01/03 19:14:20 INFO BlockManagerInfo: Added rdd_13_0 in memory on 171.17.238.3:34619 (size: 40.8 MB, free: 571.7 MB)\n",
      "19/01/03 19:14:20 INFO CodeGenerator: Code generated in 12.2393 ms\n",
      "19/01/03 19:14:20 INFO CodeGenerator: Code generated in 50.6622 ms\n",
      "19/01/03 19:14:20 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 2808 bytes result sent to driver\n",
      "19/01/03 19:14:20 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 2499 ms on localhost (executor driver) (1/1)\n",
      "19/01/03 19:14:20 INFO DAGScheduler: ShuffleMapStage 3 (count at cmd6.sc:1) finished in 2.502 s\n",
      "19/01/03 19:14:20 INFO DAGScheduler: looking for newly runnable stages\n",
      "19/01/03 19:14:20 INFO DAGScheduler: running: Set()\n",
      "19/01/03 19:14:20 INFO DAGScheduler: waiting: Set(ResultStage 4)\n",
      "19/01/03 19:14:20 INFO DAGScheduler: failed: Set()\n",
      "19/01/03 19:14:20 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[19] at count at cmd6.sc:1), which has no missing parents\n",
      "19/01/03 19:14:20 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
      "19/01/03 19:14:20 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 7.0 KB, free 571.3 MB)\n",
      "19/01/03 19:14:20 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 3.7 KB, free 571.3 MB)\n",
      "19/01/03 19:14:20 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 171.17.238.3:34619 (size: 3.7 KB, free: 571.7 MB)\n",
      "19/01/03 19:14:20 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1006\n",
      "19/01/03 19:14:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[19] at count at cmd6.sc:1) (first 15 tasks are for partitions Vector(0))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19/01/03 19:14:20 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks\n",
      "19/01/03 19:14:20 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4, localhost, executor driver, partition 0, ANY, 4726 bytes)\n",
      "19/01/03 19:14:20 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)\n",
      "19/01/03 19:14:20 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks\n",
      "19/01/03 19:14:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "19/01/03 19:14:20 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1624 bytes result sent to driver\n",
      "19/01/03 19:14:20 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 36 ms on localhost (executor driver) (1/1)\n",
      "19/01/03 19:14:20 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
      "19/01/03 19:14:20 INFO DAGScheduler: ResultStage 4 (count at cmd6.sc:1) finished in 0.038 s\n",
      "19/01/03 19:14:20 INFO DAGScheduler: Job 1 finished: count at cmd6.sc:1, took 4.610419 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27742\n"
     ]
    }
   ],
   "source": [
    "println(timezoneData.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19/01/03 19:14:21 INFO FileSourceStrategy: Pruning directories with: \n",
      "19/01/03 19:14:21 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "19/01/03 19:14:21 INFO FileSourceStrategy: Output Data Schema: struct<value: string>\n",
      "19/01/03 19:14:21 INFO FileSourceScanExec: Pushed Filters: \n",
      "19/01/03 19:14:21 INFO CodeGenerator: Code generated in 6.561 ms\n",
      "19/01/03 19:14:21 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 220.2 KB, free 571.1 MB)\n",
      "19/01/03 19:14:21 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 20.6 KB, free 571.0 MB)\n",
      "19/01/03 19:14:21 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 171.17.238.3:34619 (size: 20.6 KB, free: 571.7 MB)\n",
      "19/01/03 19:14:21 INFO ContextCleaner: Cleaned accumulator 61\n",
      "19/01/03 19:14:21 INFO ContextCleaner: Cleaned accumulator 52\n",
      "19/01/03 19:14:21 INFO ContextCleaner: Cleaned accumulator 54\n",
      "19/01/03 19:14:21 INFO ContextCleaner: Cleaned accumulator 58\n",
      "19/01/03 19:14:21 INFO ContextCleaner: Cleaned accumulator 56\n",
      "19/01/03 19:14:21 INFO ContextCleaner: Cleaned accumulator 57\n",
      "19/01/03 19:14:21 INFO ContextCleaner: Cleaned accumulator 53\n",
      "19/01/03 19:14:21 INFO ContextCleaner: Cleaned accumulator 62\n",
      "19/01/03 19:14:21 INFO ContextCleaner: Cleaned accumulator 59\n",
      "19/01/03 19:14:21 INFO SparkContext: Created broadcast 8 from json at cmd7.sc:1\n",
      "19/01/03 19:14:21 INFO ContextCleaner: Cleaned shuffle 0\n",
      "19/01/03 19:14:21 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 171.17.238.3:34619 in memory (size: 13.6 KB, free: 571.7 MB)\n",
      "19/01/03 19:14:21 INFO ContextCleaner: Cleaned accumulator 63\n",
      "19/01/03 19:14:21 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 171.17.238.3:34619 in memory (size: 3.7 KB, free: 571.7 MB)\n",
      "19/01/03 19:14:21 INFO ContextCleaner: Cleaned accumulator 55\n",
      "19/01/03 19:14:21 INFO ContextCleaner: Cleaned accumulator 60\n",
      "19/01/03 19:14:21 INFO FileSourceScanExec: Planning scan with bin packing, max size: 65525508 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "19/01/03 19:14:22 INFO SparkContext: Starting job: json at cmd7.sc:1\n",
      "19/01/03 19:14:22 INFO DAGScheduler: Got job 2 (json at cmd7.sc:1) with 2 output partitions\n",
      "19/01/03 19:14:22 INFO DAGScheduler: Final stage: ResultStage 5 (json at cmd7.sc:1)\n",
      "19/01/03 19:14:22 INFO DAGScheduler: Parents of final stage: List()\n",
      "19/01/03 19:14:22 INFO DAGScheduler: Missing parents: List()\n",
      "19/01/03 19:14:22 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[23] at json at cmd7.sc:1), which has no missing parents\n",
      "19/01/03 19:14:22 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 9.3 KB, free 571.1 MB)\n",
      "19/01/03 19:14:22 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 5.1 KB, free 571.1 MB)\n",
      "19/01/03 19:14:22 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 171.17.238.3:34619 (size: 5.1 KB, free: 571.7 MB)\n",
      "19/01/03 19:14:22 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1006\n",
      "19/01/03 19:14:22 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 5 (MapPartitionsRDD[23] at json at cmd7.sc:1) (first 15 tasks are for partitions Vector(0, 1))\n",
      "19/01/03 19:14:22 INFO TaskSchedulerImpl: Adding task set 5.0 with 2 tasks\n",
      "19/01/03 19:14:22 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5, localhost, executor driver, partition 0, PROCESS_LOCAL, 5312 bytes)\n",
      "19/01/03 19:14:22 INFO TaskSetManager: Starting task 1.0 in stage 5.0 (TID 6, localhost, executor driver, partition 1, PROCESS_LOCAL, 5312 bytes)\n",
      "19/01/03 19:14:22 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)\n",
      "19/01/03 19:14:22 INFO Executor: Running task 1.0 in stage 5.0 (TID 6)\n",
      "19/01/03 19:14:22 INFO FileScanRDD: Reading File path: file:///home/jovyan/work/data/meetup-rawdata/meetup-rawdata-2018.12.01.json, range: 65525508-126856713, partition values: [empty row]\n",
      "19/01/03 19:14:22 INFO FileScanRDD: Reading File path: file:///home/jovyan/work/data/meetup-rawdata/meetup-rawdata-2018.12.01.json, range: 0-65525508, partition values: [empty row]\n",
      "19/01/03 19:14:22 INFO CodeGenerator: Code generated in 31.1698 ms\n",
      "19/01/03 19:14:24 INFO Executor: Finished task 1.0 in stage 5.0 (TID 6). 3428 bytes result sent to driver\n",
      "19/01/03 19:14:24 INFO TaskSetManager: Finished task 1.0 in stage 5.0 (TID 6) in 2448 ms on localhost (executor driver) (1/2)\n",
      "19/01/03 19:14:24 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 3428 bytes result sent to driver\n",
      "19/01/03 19:14:24 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 2633 ms on localhost (executor driver) (2/2)\n",
      "19/01/03 19:14:24 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
      "19/01/03 19:14:24 INFO DAGScheduler: ResultStage 5 (json at cmd7.sc:1) finished in 2.634 s\n",
      "19/01/03 19:14:24 INFO DAGScheduler: Job 2 finished: json at cmd7.sc:1, took 2.668608 s\n",
      "19/01/03 19:14:24 INFO SparkSqlParser: Parsing command: response == 'yes'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mDataset\u001b[39m[\u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mRow\u001b[39m] = [rsvp_id: bigint, time: bigint ... 6 more fields]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.read.json(\"data/meetup-rawdata/*json\")\n",
    "    .where(\"response == 'yes'\")\n",
    "    .select(\"rsvp_id\", \"event.time\", \"mtime\", \"group.group_topics\", \"group.group_lon\", \"group.group_lat\", \"venue.lon\", \"venue.lat\")\n",
    "    .limit(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19/01/03 19:14:25 INFO FileSourceStrategy: Pruning directories with: \n",
      "19/01/03 19:14:25 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(response#87),(response#87 = yes)\n",
      "19/01/03 19:14:25 INFO FileSourceStrategy: Output Data Schema: struct<response: string>\n",
      "19/01/03 19:14:25 INFO FileSourceScanExec: Pushed Filters: IsNotNull(response),EqualTo(response,yes)\n",
      "19/01/03 19:14:25 INFO CodeGenerator: Code generated in 34.1647 ms\n",
      "19/01/03 19:14:26 INFO CodeGenerator: Code generated in 11.4488 ms\n",
      "19/01/03 19:14:26 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 220.2 KB, free 570.9 MB)\n",
      "19/01/03 19:14:26 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 20.6 KB, free 570.9 MB)\n",
      "19/01/03 19:14:26 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 171.17.238.3:34619 (size: 20.6 KB, free: 571.7 MB)\n",
      "19/01/03 19:14:26 INFO SparkContext: Created broadcast 10 from count at cmd8.sc:1\n",
      "19/01/03 19:14:26 INFO FileSourceScanExec: Planning scan with bin packing, max size: 65525508 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "19/01/03 19:14:26 INFO SparkContext: Starting job: count at cmd8.sc:1\n",
      "19/01/03 19:14:26 INFO DAGScheduler: Registering RDD 26 (count at cmd8.sc:1)\n",
      "19/01/03 19:14:26 INFO DAGScheduler: Got job 3 (count at cmd8.sc:1) with 1 output partitions\n",
      "19/01/03 19:14:26 INFO DAGScheduler: Final stage: ResultStage 7 (count at cmd8.sc:1)\n",
      "19/01/03 19:14:26 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)\n",
      "19/01/03 19:14:26 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 6)\n",
      "19/01/03 19:14:26 INFO DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[26] at count at cmd8.sc:1), which has no missing parents\n",
      "19/01/03 19:14:26 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 11.2 KB, free 570.8 MB)\n",
      "19/01/03 19:14:26 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 6.1 KB, free 570.8 MB)\n",
      "19/01/03 19:14:26 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 171.17.238.3:34619 (size: 6.1 KB, free: 571.7 MB)\n",
      "19/01/03 19:14:26 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1006\n",
      "19/01/03 19:14:26 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[26] at count at cmd8.sc:1) (first 15 tasks are for partitions Vector(0, 1))\n",
      "19/01/03 19:14:26 INFO TaskSchedulerImpl: Adding task set 6.0 with 2 tasks\n",
      "19/01/03 19:14:26 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 7, localhost, executor driver, partition 0, PROCESS_LOCAL, 5301 bytes)\n",
      "19/01/03 19:14:26 INFO TaskSetManager: Starting task 1.0 in stage 6.0 (TID 8, localhost, executor driver, partition 1, PROCESS_LOCAL, 5301 bytes)\n",
      "19/01/03 19:14:26 INFO Executor: Running task 0.0 in stage 6.0 (TID 7)\n",
      "19/01/03 19:14:26 INFO Executor: Running task 1.0 in stage 6.0 (TID 8)\n",
      "19/01/03 19:14:26 INFO FileScanRDD: Reading File path: file:///home/jovyan/work/data/meetup-rawdata/meetup-rawdata-2018.12.01.json, range: 0-65525508, partition values: [empty row]\n",
      "19/01/03 19:14:26 INFO FileScanRDD: Reading File path: file:///home/jovyan/work/data/meetup-rawdata/meetup-rawdata-2018.12.01.json, range: 65525508-126856713, partition values: [empty row]\n",
      "19/01/03 19:14:27 INFO ContextCleaner: Cleaned accumulator 160\n",
      "19/01/03 19:14:27 INFO ContextCleaner: Cleaned accumulator 164\n",
      "19/01/03 19:14:27 INFO ContextCleaner: Cleaned accumulator 161\n",
      "19/01/03 19:14:27 INFO ContextCleaner: Cleaned accumulator 163\n",
      "19/01/03 19:14:27 INFO ContextCleaner: Cleaned accumulator 189\n",
      "19/01/03 19:14:27 INFO ContextCleaner: Cleaned accumulator 162\n",
      "19/01/03 19:14:27 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 171.17.238.3:34619 in memory (size: 5.1 KB, free: 571.7 MB)\n",
      "19/01/03 19:14:27 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 171.17.238.3:34619 in memory (size: 20.6 KB, free: 571.7 MB)\n",
      "19/01/03 19:14:27 INFO Executor: Finished task 1.0 in stage 6.0 (TID 8). 1535 bytes result sent to driver\n",
      "19/01/03 19:14:27 INFO TaskSetManager: Finished task 1.0 in stage 6.0 (TID 8) in 1628 ms on localhost (executor driver) (1/2)\n",
      "19/01/03 19:14:27 INFO Executor: Finished task 0.0 in stage 6.0 (TID 7). 1535 bytes result sent to driver\n",
      "19/01/03 19:14:27 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 7) in 1684 ms on localhost (executor driver) (2/2)\n",
      "19/01/03 19:14:27 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
      "19/01/03 19:14:27 INFO DAGScheduler: ShuffleMapStage 6 (count at cmd8.sc:1) finished in 1.685 s\n",
      "19/01/03 19:14:27 INFO DAGScheduler: looking for newly runnable stages\n",
      "19/01/03 19:14:27 INFO DAGScheduler: running: Set()\n",
      "19/01/03 19:14:27 INFO DAGScheduler: waiting: Set(ResultStage 7)\n",
      "19/01/03 19:14:27 INFO DAGScheduler: failed: Set()\n",
      "19/01/03 19:14:27 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[29] at count at cmd8.sc:1), which has no missing parents\n",
      "19/01/03 19:14:27 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 8.9 KB, free 571.1 MB)\n",
      "19/01/03 19:14:27 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 4.1 KB, free 571.1 MB)\n",
      "19/01/03 19:14:27 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 171.17.238.3:34619 (size: 4.1 KB, free: 571.7 MB)\n",
      "19/01/03 19:14:27 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1006\n",
      "19/01/03 19:14:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[29] at count at cmd8.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "19/01/03 19:14:27 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks\n",
      "19/01/03 19:14:27 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 9, localhost, executor driver, partition 0, ANY, 4726 bytes)\n",
      "19/01/03 19:14:27 INFO Executor: Running task 0.0 in stage 7.0 (TID 9)\n",
      "19/01/03 19:14:27 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks\n",
      "19/01/03 19:14:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "19/01/03 19:14:27 INFO Executor: Finished task 0.0 in stage 7.0 (TID 9). 1481 bytes result sent to driver\n",
      "19/01/03 19:14:27 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 9) in 59 ms on localhost (executor driver) (1/1)\n",
      "19/01/03 19:14:27 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool \n",
      "19/01/03 19:14:27 INFO DAGScheduler: ResultStage 7 (count at cmd8.sc:1) finished in 0.060 s\n",
      "19/01/03 19:14:27 INFO DAGScheduler: Job 3 finished: count at cmd8.sc:1, took 1.795864 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "println(df.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- rsvp_id: long (nullable = true)\n",
      " |-- time: long (nullable = true)\n",
      " |-- mtime: long (nullable = true)\n",
      " |-- group_topics: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- group_lon: double (nullable = true)\n",
      " |-- group_lat: double (nullable = true)\n",
      " |-- lon: double (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select only last response for each rsvp_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19/01/03 19:14:28 INFO SparkSqlParser: Parsing command: rowId = 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.expressions.Window\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions.{row_number, col, desc}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mmagellan.{Point, Polygon}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.magellan.dsl.expressions._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.types._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mrsvpWindowSpec\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mexpressions\u001b[39m.\u001b[32mWindowSpec\u001b[39m = org.apache.spark.sql.expressions.WindowSpec@625e8645\n",
       "\u001b[36mdfLastId\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mpackage\u001b[39m.\u001b[32mDataFrame\u001b[39m = [rsvp_id: bigint, time: bigint ... 9 more fields]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.expressions.Window\n",
    "import org.apache.spark.sql.functions.{row_number, col, desc}\n",
    "import magellan.{Point, Polygon}\n",
    "import org.apache.spark.sql.magellan.dsl.expressions._\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "val rsvpWindowSpec = Window.partitionBy(\"rsvp_id\").orderBy(desc(\"mtime\"))\n",
    "\n",
    "val dfLastId = df.withColumn(\"rowId\", row_number().over(rsvpWindowSpec))\n",
    "    .where(\"rowId = 1\")\n",
    "    .orderBy(\"rsvp_id\")\n",
    "    .withColumn(\"venuePoint\", point(col(\"lat\"), col(\"lon\")))\n",
    "    .withColumn(\"groupPoint\", point(col(\"group_lat\"), col(\"group_lon\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19/01/03 19:14:29 INFO FileSourceStrategy: Pruning directories with: \n",
      "19/01/03 19:14:29 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(response#87),(response#87 = yes)\n",
      "19/01/03 19:14:29 INFO FileSourceStrategy: Output Data Schema: struct<mtime: bigint, response: string, rsvp_id: bigint ... 1 more fields>\n",
      "19/01/03 19:14:29 INFO FileSourceScanExec: Pushed Filters: IsNotNull(response),EqualTo(response,yes)\n",
      "19/01/03 19:14:29 INFO CodeGenerator: Code generated in 39.1914 ms\n",
      "19/01/03 19:14:29 INFO CodeGenerator: Code generated in 22.3309 ms\n",
      "19/01/03 19:14:29 INFO CodeGenerator: Code generated in 24.2342 ms\n",
      "19/01/03 19:14:29 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 220.2 KB, free 570.9 MB)\n",
      "19/01/03 19:14:29 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 20.6 KB, free 570.8 MB)\n",
      "19/01/03 19:14:29 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 171.17.238.3:34619 (size: 20.6 KB, free: 571.7 MB)\n",
      "19/01/03 19:14:29 INFO SparkContext: Created broadcast 13 from count at cmd11.sc:1\n",
      "19/01/03 19:14:29 INFO FileSourceScanExec: Planning scan with bin packing, max size: 65525508 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "19/01/03 19:14:29 INFO SparkContext: Starting job: count at cmd11.sc:1\n",
      "19/01/03 19:14:29 INFO DAGScheduler: Registering RDD 32 (count at cmd11.sc:1)\n",
      "19/01/03 19:14:29 INFO DAGScheduler: Got job 4 (count at cmd11.sc:1) with 1 output partitions\n",
      "19/01/03 19:14:29 INFO DAGScheduler: Final stage: ResultStage 9 (count at cmd11.sc:1)\n",
      "19/01/03 19:14:29 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 8)\n",
      "19/01/03 19:14:29 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 8)\n",
      "19/01/03 19:14:29 INFO DAGScheduler: Submitting ShuffleMapStage 8 (MapPartitionsRDD[32] at count at cmd11.sc:1), which has no missing parents\n",
      "19/01/03 19:14:29 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 12.9 KB, free 570.8 MB)\n",
      "19/01/03 19:14:29 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 6.5 KB, free 570.8 MB)\n",
      "19/01/03 19:14:29 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 171.17.238.3:34619 (size: 6.5 KB, free: 571.7 MB)\n",
      "19/01/03 19:14:29 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1006\n",
      "19/01/03 19:14:29 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 8 (MapPartitionsRDD[32] at count at cmd11.sc:1) (first 15 tasks are for partitions Vector(0, 1))\n",
      "19/01/03 19:14:29 INFO TaskSchedulerImpl: Adding task set 8.0 with 2 tasks\n",
      "19/01/03 19:14:29 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 10, localhost, executor driver, partition 0, PROCESS_LOCAL, 5301 bytes)\n",
      "19/01/03 19:14:29 INFO TaskSetManager: Starting task 1.0 in stage 8.0 (TID 11, localhost, executor driver, partition 1, PROCESS_LOCAL, 5301 bytes)\n",
      "19/01/03 19:14:29 INFO Executor: Running task 1.0 in stage 8.0 (TID 11)\n",
      "19/01/03 19:14:29 INFO Executor: Running task 0.0 in stage 8.0 (TID 10)\n",
      "19/01/03 19:14:29 INFO FileScanRDD: Reading File path: file:///home/jovyan/work/data/meetup-rawdata/meetup-rawdata-2018.12.01.json, range: 65525508-126856713, partition values: [empty row]\n",
      "19/01/03 19:14:29 INFO FileScanRDD: Reading File path: file:///home/jovyan/work/data/meetup-rawdata/meetup-rawdata-2018.12.01.json, range: 0-65525508, partition values: [empty row]\n",
      "19/01/03 19:14:29 INFO CodeGenerator: Code generated in 22.9648 ms\n",
      "19/01/03 19:14:30 INFO ContextCleaner: Cleaned accumulator 205\n",
      "19/01/03 19:14:30 INFO ContextCleaner: Cleaned accumulator 197\n",
      "19/01/03 19:14:30 INFO ContextCleaner: Cleaned accumulator 190\n",
      "19/01/03 19:14:30 INFO ContextCleaner: Cleaned accumulator 194\n",
      "19/01/03 19:14:30 INFO ContextCleaner: Cleaned accumulator 193\n",
      "19/01/03 19:14:30 INFO ContextCleaner: Cleaned accumulator 196\n",
      "19/01/03 19:14:30 INFO ContextCleaner: Cleaned accumulator 199\n",
      "19/01/03 19:14:30 INFO ContextCleaner: Cleaned accumulator 202\n",
      "19/01/03 19:14:30 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 171.17.238.3:34619 in memory (size: 20.6 KB, free: 571.7 MB)\n",
      "19/01/03 19:14:30 INFO ContextCleaner: Cleaned accumulator 204\n",
      "19/01/03 19:14:30 INFO ContextCleaner: Cleaned accumulator 198\n",
      "19/01/03 19:14:30 INFO ContextCleaner: Cleaned accumulator 192\n",
      "19/01/03 19:14:30 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 171.17.238.3:34619 in memory (size: 4.1 KB, free: 571.7 MB)\n",
      "19/01/03 19:14:30 INFO ContextCleaner: Cleaned accumulator 200\n",
      "19/01/03 19:14:30 INFO ContextCleaner: Cleaned accumulator 195\n",
      "19/01/03 19:14:30 INFO ContextCleaner: Cleaned shuffle 3\n",
      "19/01/03 19:14:30 INFO ContextCleaner: Cleaned accumulator 203\n",
      "19/01/03 19:14:30 INFO ContextCleaner: Cleaned accumulator 191\n",
      "19/01/03 19:14:30 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 171.17.238.3:34619 in memory (size: 6.1 KB, free: 571.7 MB)\n",
      "19/01/03 19:14:30 INFO ContextCleaner: Cleaned accumulator 201\n",
      "19/01/03 19:14:30 INFO ContextCleaner: Cleaned accumulator 254\n",
      "19/01/03 19:14:30 INFO Executor: Finished task 1.0 in stage 8.0 (TID 11). 1578 bytes result sent to driver\n",
      "19/01/03 19:14:30 INFO TaskSetManager: Finished task 1.0 in stage 8.0 (TID 11) in 1117 ms on localhost (executor driver) (1/2)\n",
      "19/01/03 19:14:30 INFO Executor: Finished task 0.0 in stage 8.0 (TID 10). 1535 bytes result sent to driver\n",
      "19/01/03 19:14:30 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 10) in 1305 ms on localhost (executor driver) (2/2)\n",
      "19/01/03 19:14:30 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool \n",
      "19/01/03 19:14:30 INFO DAGScheduler: ShuffleMapStage 8 (count at cmd11.sc:1) finished in 1.291 s\n",
      "19/01/03 19:14:30 INFO DAGScheduler: looking for newly runnable stages\n",
      "19/01/03 19:14:30 INFO DAGScheduler: running: Set()\n",
      "19/01/03 19:14:30 INFO DAGScheduler: waiting: Set(ResultStage 9)\n",
      "19/01/03 19:14:30 INFO DAGScheduler: failed: Set()\n",
      "19/01/03 19:14:30 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[37] at count at cmd11.sc:1), which has no missing parents\n",
      "19/01/03 19:14:30 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 34.3 KB, free 571.0 MB)\n",
      "19/01/03 19:14:30 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 14.4 KB, free 571.0 MB)\n",
      "19/01/03 19:14:30 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 171.17.238.3:34619 (size: 14.4 KB, free: 571.7 MB)\n",
      "19/01/03 19:14:30 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1006\n",
      "19/01/03 19:14:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[37] at count at cmd11.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "19/01/03 19:14:30 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks\n",
      "19/01/03 19:14:30 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 12, localhost, executor driver, partition 0, ANY, 4726 bytes)\n",
      "19/01/03 19:14:30 INFO Executor: Running task 0.0 in stage 9.0 (TID 12)\n",
      "19/01/03 19:14:30 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks\n",
      "19/01/03 19:14:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms\n",
      "19/01/03 19:14:30 INFO CodeGenerator: Code generated in 12.3657 ms\n",
      "19/01/03 19:14:30 INFO CodeGenerator: Code generated in 5.8032 ms\n",
      "19/01/03 19:14:31 INFO CodeGenerator: Code generated in 7.6289 ms\n",
      "19/01/03 19:14:31 INFO CodeGenerator: Code generated in 7.3965 ms\n",
      "19/01/03 19:14:31 INFO CodeGenerator: Code generated in 5.5144 ms\n",
      "19/01/03 19:14:31 INFO CodeGenerator: Code generated in 13.3749 ms\n",
      "19/01/03 19:14:31 INFO CodeGenerator: Code generated in 7.0399 ms\n",
      "19/01/03 19:14:31 INFO CodeGenerator: Code generated in 15.4451 ms\n",
      "19/01/03 19:14:31 INFO Executor: Finished task 0.0 in stage 9.0 (TID 12). 3056 bytes result sent to driver\n",
      "19/01/03 19:14:31 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 12) in 294 ms on localhost (executor driver) (1/1)\n",
      "19/01/03 19:14:31 INFO DAGScheduler: ResultStage 9 (count at cmd11.sc:1) finished in 0.296 s\n",
      "19/01/03 19:14:31 INFO DAGScheduler: Job 4 finished: count at cmd11.sc:1, took 1.659877 s\n",
      "19/01/03 19:14:31 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mres11\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m857L\u001b[39m"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfLastId.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Establish timezone, day_of_week_local, hour_local, minute_local of event.event_time based on venue.venue_geo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19/01/03 19:14:32 INFO FileSourceStrategy: Pruning directories with: \n",
      "19/01/03 19:14:32 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(response#87),(response#87 = yes)\n",
      "19/01/03 19:14:32 INFO FileSourceStrategy: Output Data Schema: struct<event: struct<event_id: string, event_name: string, event_url: string, time: bigint ... 2 more fields>, group: struct<group_city: string, group_country: string, group_geo: string, group_id: bigint, group_lat: double ... 8 more fields>, mtime: bigint, response: string, rsvp_id: bigint ... 1 more field>\n",
      "19/01/03 19:14:32 INFO FileSourceScanExec: Pushed Filters: IsNotNull(response),EqualTo(response,yes)\n",
      "19/01/03 19:14:32 INFO CodeGenerator: Code generated in 24.9144 ms\n",
      "19/01/03 19:14:32 INFO CodeGenerator: Code generated in 19.9885 ms\n",
      "19/01/03 19:14:32 INFO CodeGenerator: Code generated in 17.8034 ms\n",
      "19/01/03 19:14:32 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 220.2 KB, free 570.8 MB)\n",
      "19/01/03 19:14:32 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 20.6 KB, free 570.8 MB)\n",
      "19/01/03 19:14:32 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 171.17.238.3:34619 (size: 20.6 KB, free: 571.7 MB)\n",
      "19/01/03 19:14:32 INFO SparkContext: Created broadcast 16 from run at ThreadPoolExecutor.java:1149\n",
      "19/01/03 19:14:32 INFO FileSourceScanExec: Planning scan with bin packing, max size: 65525508 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "19/01/03 19:14:32 INFO SparkContext: Starting job: run at ThreadPoolExecutor.java:1149\n",
      "19/01/03 19:14:32 INFO DAGScheduler: Registering RDD 40 (run at ThreadPoolExecutor.java:1149)\n",
      "19/01/03 19:14:32 INFO DAGScheduler: Got job 5 (run at ThreadPoolExecutor.java:1149) with 1 output partitions\n",
      "19/01/03 19:14:32 INFO DAGScheduler: Final stage: ResultStage 11 (run at ThreadPoolExecutor.java:1149)\n",
      "19/01/03 19:14:32 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 10)\n",
      "19/01/03 19:14:32 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 10)\n",
      "19/01/03 19:14:32 INFO DAGScheduler: Submitting ShuffleMapStage 10 (MapPartitionsRDD[40] at run at ThreadPoolExecutor.java:1149), which has no missing parents\n",
      "19/01/03 19:14:32 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 19.0 KB, free 570.8 MB)\n",
      "19/01/03 19:14:32 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 8.5 KB, free 570.8 MB)\n",
      "19/01/03 19:14:32 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 171.17.238.3:34619 (size: 8.5 KB, free: 571.7 MB)\n",
      "19/01/03 19:14:32 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1006\n",
      "19/01/03 19:14:32 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[40] at run at ThreadPoolExecutor.java:1149) (first 15 tasks are for partitions Vector(0, 1))\n",
      "19/01/03 19:14:32 INFO TaskSchedulerImpl: Adding task set 10.0 with 2 tasks\n",
      "19/01/03 19:14:32 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 13, localhost, executor driver, partition 0, PROCESS_LOCAL, 5301 bytes)\n",
      "19/01/03 19:14:32 INFO TaskSetManager: Starting task 1.0 in stage 10.0 (TID 14, localhost, executor driver, partition 1, PROCESS_LOCAL, 5301 bytes)\n",
      "19/01/03 19:14:32 INFO Executor: Running task 0.0 in stage 10.0 (TID 13)\n",
      "19/01/03 19:14:32 INFO Executor: Running task 1.0 in stage 10.0 (TID 14)\n",
      "19/01/03 19:14:32 INFO FileScanRDD: Reading File path: file:///home/jovyan/work/data/meetup-rawdata/meetup-rawdata-2018.12.01.json, range: 0-65525508, partition values: [empty row]\n",
      "19/01/03 19:14:32 INFO FileScanRDD: Reading File path: file:///home/jovyan/work/data/meetup-rawdata/meetup-rawdata-2018.12.01.json, range: 65525508-126856713, partition values: [empty row]\n",
      "19/01/03 19:14:32 INFO CodeGenerator: Code generated in 38.3709 ms\n",
      "19/01/03 19:14:32 INFO ContextCleaner: Cleaned accumulator 278\n",
      "19/01/03 19:14:32 INFO ContextCleaner: Cleaned accumulator 272\n",
      "19/01/03 19:14:32 INFO ContextCleaner: Cleaned accumulator 270\n",
      "19/01/03 19:14:32 INFO ContextCleaner: Cleaned accumulator 261\n",
      "19/01/03 19:14:32 INFO ContextCleaner: Cleaned accumulator 268\n",
      "19/01/03 19:14:32 INFO ContextCleaner: Cleaned accumulator 267\n",
      "19/01/03 19:14:32 INFO ContextCleaner: Cleaned accumulator 257\n",
      "19/01/03 19:14:32 INFO ContextCleaner: Cleaned accumulator 271\n",
      "19/01/03 19:14:32 INFO ContextCleaner: Cleaned accumulator 275\n",
      "19/01/03 19:14:32 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 171.17.238.3:34619 in memory (size: 20.6 KB, free: 571.7 MB)\n",
      "19/01/03 19:14:32 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 171.17.238.3:34619 in memory (size: 6.5 KB, free: 571.7 MB)\n",
      "19/01/03 19:14:32 INFO ContextCleaner: Cleaned accumulator 327\n",
      "19/01/03 19:14:32 INFO ContextCleaner: Cleaned accumulator 263\n",
      "19/01/03 19:14:32 INFO ContextCleaner: Cleaned accumulator 260\n",
      "19/01/03 19:14:32 INFO ContextCleaner: Cleaned accumulator 264\n",
      "19/01/03 19:14:32 INFO ContextCleaner: Cleaned accumulator 262\n",
      "19/01/03 19:14:32 INFO ContextCleaner: Cleaned accumulator 274\n",
      "19/01/03 19:14:32 INFO ContextCleaner: Cleaned accumulator 256\n",
      "19/01/03 19:14:32 INFO ContextCleaner: Cleaned accumulator 255\n",
      "19/01/03 19:14:32 INFO ContextCleaner: Cleaned accumulator 259\n",
      "19/01/03 19:14:32 INFO ContextCleaner: Cleaned accumulator 269\n",
      "19/01/03 19:14:32 INFO ContextCleaner: Cleaned accumulator 273\n",
      "19/01/03 19:14:32 INFO ContextCleaner: Cleaned shuffle 4\n",
      "19/01/03 19:14:32 INFO ContextCleaner: Cleaned accumulator 258\n",
      "19/01/03 19:14:32 INFO ContextCleaner: Cleaned accumulator 265\n",
      "19/01/03 19:14:32 INFO ContextCleaner: Cleaned accumulator 266\n",
      "19/01/03 19:14:32 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 171.17.238.3:34619 in memory (size: 14.4 KB, free: 571.7 MB)\n",
      "19/01/03 19:14:32 INFO ContextCleaner: Cleaned accumulator 277\n",
      "19/01/03 19:14:32 INFO ContextCleaner: Cleaned accumulator 276\n",
      "19/01/03 19:14:34 INFO Executor: Finished task 1.0 in stage 10.0 (TID 14). 1535 bytes result sent to driver\n",
      "19/01/03 19:14:34 INFO TaskSetManager: Finished task 1.0 in stage 10.0 (TID 14) in 2006 ms on localhost (executor driver) (1/2)\n",
      "19/01/03 19:14:34 INFO Executor: Finished task 0.0 in stage 10.0 (TID 13). 1535 bytes result sent to driver\n",
      "19/01/03 19:14:34 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 13) in 2175 ms on localhost (executor driver) (2/2)\n",
      "19/01/03 19:14:34 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool \n",
      "19/01/03 19:14:34 INFO DAGScheduler: ShuffleMapStage 10 (run at ThreadPoolExecutor.java:1149) finished in 2.175 s\n",
      "19/01/03 19:14:34 INFO DAGScheduler: looking for newly runnable stages\n",
      "19/01/03 19:14:34 INFO DAGScheduler: running: Set()\n",
      "19/01/03 19:14:34 INFO DAGScheduler: waiting: Set(ResultStage 11)\n",
      "19/01/03 19:14:34 INFO DAGScheduler: failed: Set()\n",
      "19/01/03 19:14:34 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[46] at run at ThreadPoolExecutor.java:1149), which has no missing parents\n",
      "19/01/03 19:14:34 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 48.6 KB, free 571.0 MB)\n",
      "19/01/03 19:14:34 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 19.1 KB, free 571.0 MB)\n",
      "19/01/03 19:14:34 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 171.17.238.3:34619 (size: 19.1 KB, free: 571.7 MB)\n",
      "19/01/03 19:14:34 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1006\n",
      "19/01/03 19:14:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[46] at run at ThreadPoolExecutor.java:1149) (first 15 tasks are for partitions Vector(0))\n",
      "19/01/03 19:14:34 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks\n",
      "19/01/03 19:14:34 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 15, localhost, executor driver, partition 0, ANY, 4726 bytes)\n",
      "19/01/03 19:14:34 INFO Executor: Running task 0.0 in stage 11.0 (TID 15)\n",
      "19/01/03 19:14:34 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks\n",
      "19/01/03 19:14:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "19/01/03 19:14:34 INFO CodeGenerator: Code generated in 12.8457 ms\n",
      "19/01/03 19:14:34 INFO CodeGenerator: Code generated in 25.151 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19/01/03 19:14:34 INFO CodeGenerator: Code generated in 45.9414 ms\n",
      "19/01/03 19:14:34 INFO Executor: Finished task 0.0 in stage 11.0 (TID 15). 264305 bytes result sent to driver\n",
      "19/01/03 19:14:34 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 15) in 257 ms on localhost (executor driver) (1/1)\n",
      "19/01/03 19:14:34 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool \n",
      "19/01/03 19:14:34 INFO DAGScheduler: ResultStage 11 (run at ThreadPoolExecutor.java:1149) finished in 0.257 s\n",
      "19/01/03 19:14:34 INFO DAGScheduler: Job 5 finished: run at ThreadPoolExecutor.java:1149, took 2.484701 s\n",
      "19/01/03 19:14:34 INFO CodeGenerator: Code generated in 9.7671 ms\n",
      "19/01/03 19:14:34 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 16.0 MB, free 555.0 MB)\n",
      "19/01/03 19:14:34 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 274.4 KB, free 554.7 MB)\n",
      "19/01/03 19:14:34 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 171.17.238.3:34619 (size: 274.4 KB, free: 571.4 MB)\n",
      "19/01/03 19:14:34 INFO SparkContext: Created broadcast 19 from run at ThreadPoolExecutor.java:1149\n",
      "19/01/03 19:14:35 INFO CodeGenerator: Code generated in 100.9983 ms\n",
      "19/01/03 19:14:35 INFO FileSourceStrategy: Pruning directories with: \n",
      "19/01/03 19:14:35 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(response#87),(response#87 = yes)\n",
      "19/01/03 19:14:35 INFO FileSourceStrategy: Output Data Schema: struct<event: struct<event_id: string, event_name: string, event_url: string, time: bigint ... 2 more fields>, group: struct<group_city: string, group_country: string, group_geo: string, group_id: bigint, group_lat: double ... 8 more fields>, mtime: bigint, response: string, rsvp_id: bigint ... 1 more field>\n",
      "19/01/03 19:14:35 INFO FileSourceScanExec: Pushed Filters: IsNotNull(response),EqualTo(response,yes)\n",
      "19/01/03 19:14:35 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 220.2 KB, free 554.5 MB)\n",
      "19/01/03 19:14:35 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 20.6 KB, free 554.5 MB)\n",
      "19/01/03 19:14:35 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 171.17.238.3:34619 (size: 20.6 KB, free: 571.4 MB)\n",
      "19/01/03 19:14:35 INFO SparkContext: Created broadcast 20 from run at ThreadPoolExecutor.java:1149\n",
      "19/01/03 19:14:35 INFO FileSourceScanExec: Planning scan with bin packing, max size: 65525508 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "19/01/03 19:14:35 INFO SparkContext: Starting job: run at ThreadPoolExecutor.java:1149\n",
      "19/01/03 19:14:35 INFO DAGScheduler: Registering RDD 53 (run at ThreadPoolExecutor.java:1149)\n",
      "19/01/03 19:14:35 INFO DAGScheduler: Got job 6 (run at ThreadPoolExecutor.java:1149) with 1 output partitions\n",
      "19/01/03 19:14:35 INFO DAGScheduler: Final stage: ResultStage 13 (run at ThreadPoolExecutor.java:1149)\n",
      "19/01/03 19:14:35 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 12)\n",
      "19/01/03 19:14:35 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 12)\n",
      "19/01/03 19:14:35 INFO DAGScheduler: Submitting ShuffleMapStage 12 (MapPartitionsRDD[53] at run at ThreadPoolExecutor.java:1149), which has no missing parents\n",
      "19/01/03 19:14:35 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 19.0 KB, free 554.5 MB)\n",
      "19/01/03 19:14:35 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 8.5 KB, free 554.4 MB)\n",
      "19/01/03 19:14:35 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 171.17.238.3:34619 (size: 8.5 KB, free: 571.4 MB)\n",
      "19/01/03 19:14:35 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1006\n",
      "19/01/03 19:14:35 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 12 (MapPartitionsRDD[53] at run at ThreadPoolExecutor.java:1149) (first 15 tasks are for partitions Vector(0, 1))\n",
      "19/01/03 19:14:35 INFO TaskSchedulerImpl: Adding task set 12.0 with 2 tasks\n",
      "19/01/03 19:14:35 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 16, localhost, executor driver, partition 0, PROCESS_LOCAL, 5301 bytes)\n",
      "19/01/03 19:14:35 INFO TaskSetManager: Starting task 1.0 in stage 12.0 (TID 17, localhost, executor driver, partition 1, PROCESS_LOCAL, 5301 bytes)\n",
      "19/01/03 19:14:35 INFO Executor: Running task 0.0 in stage 12.0 (TID 16)\n",
      "19/01/03 19:14:35 INFO FileScanRDD: Reading File path: file:///home/jovyan/work/data/meetup-rawdata/meetup-rawdata-2018.12.01.json, range: 0-65525508, partition values: [empty row]\n",
      "19/01/03 19:14:35 INFO Executor: Running task 1.0 in stage 12.0 (TID 17)\n",
      "19/01/03 19:14:35 INFO FileScanRDD: Reading File path: file:///home/jovyan/work/data/meetup-rawdata/meetup-rawdata-2018.12.01.json, range: 65525508-126856713, partition values: [empty row]\n",
      "19/01/03 19:14:35 INFO ContextCleaner: Cleaned accumulator 402\n",
      "19/01/03 19:14:35 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 171.17.238.3:34619 in memory (size: 19.1 KB, free: 571.4 MB)\n",
      "19/01/03 19:14:35 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 171.17.238.3:34619 in memory (size: 8.5 KB, free: 571.4 MB)\n",
      "19/01/03 19:14:37 INFO Executor: Finished task 1.0 in stage 12.0 (TID 17). 1535 bytes result sent to driver\n",
      "19/01/03 19:14:37 INFO TaskSetManager: Finished task 1.0 in stage 12.0 (TID 17) in 1778 ms on localhost (executor driver) (1/2)\n",
      "19/01/03 19:14:37 INFO Executor: Finished task 0.0 in stage 12.0 (TID 16). 1535 bytes result sent to driver\n",
      "19/01/03 19:14:37 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 16) in 1928 ms on localhost (executor driver) (2/2)\n",
      "19/01/03 19:14:37 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool \n",
      "19/01/03 19:14:37 INFO DAGScheduler: ShuffleMapStage 12 (run at ThreadPoolExecutor.java:1149) finished in 1.929 s\n",
      "19/01/03 19:14:37 INFO DAGScheduler: looking for newly runnable stages\n",
      "19/01/03 19:14:37 INFO DAGScheduler: running: Set()\n",
      "19/01/03 19:14:37 INFO DAGScheduler: waiting: Set(ResultStage 13)\n",
      "19/01/03 19:14:37 INFO DAGScheduler: failed: Set()\n",
      "19/01/03 19:14:37 INFO DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[59] at run at ThreadPoolExecutor.java:1149), which has no missing parents\n",
      "19/01/03 19:14:37 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 48.6 KB, free 554.5 MB)\n",
      "19/01/03 19:14:37 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 19.1 KB, free 554.5 MB)\n",
      "19/01/03 19:14:37 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 171.17.238.3:34619 (size: 19.1 KB, free: 571.4 MB)\n",
      "19/01/03 19:14:37 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1006\n",
      "19/01/03 19:14:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[59] at run at ThreadPoolExecutor.java:1149) (first 15 tasks are for partitions Vector(0))\n",
      "19/01/03 19:14:37 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks\n",
      "19/01/03 19:14:37 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 18, localhost, executor driver, partition 0, ANY, 4726 bytes)\n",
      "19/01/03 19:14:37 INFO Executor: Running task 0.0 in stage 13.0 (TID 18)\n",
      "19/01/03 19:14:37 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks\n",
      "19/01/03 19:14:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "19/01/03 19:14:37 INFO Executor: Finished task 0.0 in stage 13.0 (TID 18). 264405 bytes result sent to driver\n",
      "19/01/03 19:14:37 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 18) in 85 ms on localhost (executor driver) (1/1)\n",
      "19/01/03 19:14:37 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool \n",
      "19/01/03 19:14:37 INFO DAGScheduler: ResultStage 13 (run at ThreadPoolExecutor.java:1149) finished in 0.085 s\n",
      "19/01/03 19:14:37 INFO DAGScheduler: Job 6 finished: run at ThreadPoolExecutor.java:1149, took 2.087096 s\n",
      "19/01/03 19:14:37 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 16.0 MB, free 538.4 MB)\n",
      "19/01/03 19:14:37 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 274.6 KB, free 538.2 MB)\n",
      "19/01/03 19:14:37 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 171.17.238.3:34619 (size: 274.6 KB, free: 571.2 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19/01/03 19:14:37 INFO SparkContext: Created broadcast 23 from run at ThreadPoolExecutor.java:1149\n",
      "19/01/03 19:14:37 INFO CodeGenerator: Code generated in 65.9928 ms\n",
      "19/01/03 19:14:37 WARN CacheManager: Asked to cache already cached data.\n",
      "19/01/03 19:14:37 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions.{concat, lit}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.magellan.dsl.expressions._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.types._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mmagellan.{Point, Polygon}\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mdfLastIdwithVenueGeo\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mDataset\u001b[39m[\u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mRow\u001b[39m] = [rsvp_id: bigint, time: bigint ... 14 more fields]\n",
       "\u001b[36mdfLastIdwithMemberGeo\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mDataset\u001b[39m[\u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mRow\u001b[39m] = [rsvp_id: bigint, time: bigint ... 14 more fields]\n",
       "\u001b[36mres12_7\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mDataset\u001b[39m[\u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mRow\u001b[39m] = [rsvp_id: bigint, time: bigint ... 14 more fields]\n",
       "\u001b[36mres12_8\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mDataset\u001b[39m[\u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mRow\u001b[39m] = [rsvp_id: bigint, time: bigint ... 14 more fields]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// to optimize matching event.time with venue.lat/venue.lon create dict with distinct venues\n",
    "import org.apache.spark.sql.functions.{concat, lit}\n",
    "import org.apache.spark.sql.magellan.dsl.expressions._\n",
    "import org.apache.spark.sql.types._\n",
    "import magellan.{Point, Polygon}\n",
    "\n",
    "magellan.Utils.injectRules(spark)\n",
    "\n",
    "val dfLastIdwithVenueGeo = dfLastId\n",
    "    .join(timezoneData)\n",
    "    .where(col(\"venuePoint\") within col(\"polygon\"))\n",
    "    .cache()\n",
    "\n",
    "val dfLastIdwithMemberGeo = dfLastId\n",
    "    .join(timezoneData)\n",
    "    .where(col(\"groupPoint\") within col(\"polygon\"))\n",
    "    .cache()\n",
    "\n",
    "dfLastIdwithVenueGeo.cache()\n",
    "dfLastIdwithMemberGeo.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19/01/03 19:14:38 INFO CodeGenerator: Code generated in 40.3405 ms\n",
      "19/01/03 19:14:38 INFO SparkContext: Starting job: show at cmd13.sc:1\n",
      "19/01/03 19:14:38 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 2 is 147 bytes\n",
      "19/01/03 19:14:38 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 1 is 148 bytes\n",
      "19/01/03 19:14:38 INFO DAGScheduler: Got job 7 (show at cmd13.sc:1) with 1 output partitions\n",
      "19/01/03 19:14:38 INFO DAGScheduler: Final stage: ResultStage 16 (show at cmd13.sc:1)\n",
      "19/01/03 19:14:38 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 15, ShuffleMapStage 14)\n",
      "19/01/03 19:14:38 INFO DAGScheduler: Missing parents: List()\n",
      "19/01/03 19:14:38 INFO DAGScheduler: Submitting ResultStage 16 (MapPartitionsRDD[65] at show at cmd13.sc:1), which has no missing parents\n",
      "19/01/03 19:14:38 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 86.1 KB, free 538.1 MB)\n",
      "19/01/03 19:14:38 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 26.7 KB, free 538.1 MB)\n",
      "19/01/03 19:14:38 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 171.17.238.3:34619 (size: 26.7 KB, free: 571.1 MB)\n",
      "19/01/03 19:14:38 INFO SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1006\n",
      "19/01/03 19:14:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 16 (MapPartitionsRDD[65] at show at cmd13.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "19/01/03 19:14:38 INFO TaskSchedulerImpl: Adding task set 16.0 with 1 tasks\n",
      "19/01/03 19:14:38 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 19, localhost, executor driver, partition 0, PROCESS_LOCAL, 4684 bytes)\n",
      "19/01/03 19:14:38 INFO Executor: Running task 0.0 in stage 16.0 (TID 19)\n",
      "19/01/03 19:14:38 INFO BlockManager: Found block rdd_13_0 locally\n",
      "19/01/03 19:14:38 INFO CodeGenerator: Code generated in 37.5039 ms\n",
      "19/01/03 19:14:38 INFO CodeGenerator: Code generated in 67.6937 ms\n",
      "19/01/03 19:14:38 INFO BlockManagerInfo: Removed broadcast_22_piece0 on 171.17.238.3:34619 in memory (size: 19.1 KB, free: 571.1 MB)\n",
      "19/01/03 19:14:38 INFO BlockManagerInfo: Removed broadcast_21_piece0 on 171.17.238.3:34619 in memory (size: 8.5 KB, free: 571.2 MB)\n",
      "19/01/03 19:39:00 WARN BlockManager: Putting block rdd_50_0 failed due to an exception\n",
      "19/01/03 19:39:00 WARN BlockManager: Block rdd_50_0 could not be removed as it was not found on disk or in memory\n",
      "19/01/03 19:39:00 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [10 seconds]. This timeout is controlled by spark.executor.heartbeatInterval\n",
      "\tat org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:76)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:92)\n",
      "\tat org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:738)\n",
      "\tat org.apache.spark.executor.Executor$$anon$2$$anonfun$run$1.apply$mcV$sp(Executor.scala:767)\n",
      "\tat org.apache.spark.executor.Executor$$anon$2$$anonfun$run$1.apply(Executor.scala:767)\n",
      "\tat org.apache.spark.executor.Executor$$anon$2$$anonfun$run$1.apply(Executor.scala:767)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1948)\n",
      "\tat org.apache.spark.executor.Executor$$anon$2.run(Executor.scala:767)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.util.concurrent.TimeoutException: Futures timed out after [10 seconds]\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:223)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:227)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:201)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\t... 14 more\n",
      "19/01/03 19:39:00 WARN NettyRpcEnv: Ignored message: HeartbeatResponse(false)\n",
      "19/01/03 19:39:00 ERROR Executor: Exception in task 0.0 in stage 16.0 (TID 19)\n",
      "java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
      "\tat java.lang.Double.valueOf(Double.java:519)\n",
      "\tat scala.runtime.BoxesRunTime.boxToDouble(BoxesRunTime.java:79)\n",
      "\tat magellan.index.ZOrderCurve.<init>(ZOrderCurve.scala:28)\n",
      "\tat magellan.index.ZOrderCurveIndexer.index(ZOrderCurveIndexer.scala:63)\n",
      "\tat magellan.index.ZOrderCurveIndexer.cover(ZOrderCurveIndexer.scala:112)\n",
      "\tat magellan.index.ZOrderCurveIndexer.indexWithMeta(ZOrderCurveIndexer.scala:74)\n",
      "\tat org.apache.spark.sql.types.Indexer.nullSafeEval(Indexer.scala:43)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.UnaryExpression.eval(Expression.scala:331)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.Inline.eval(generators.scala:374)\n",
      "\tat org.apache.spark.sql.execution.GenerateExec$$anonfun$1$$anonfun$2.apply(GenerateExec.scala:93)\n",
      "\tat org.apache.spark.sql.execution.GenerateExec$$anonfun$1$$anonfun$2.apply(GenerateExec.scala:90)\n",
      "\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)\n",
      "\tat scala.collection.Iterator$JoinIterator.hasNext(Iterator.scala:212)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:395)\n",
      "\tat org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$1$$anon$1.hasNext(InMemoryRelation.scala:133)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:215)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1038)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1029)\n",
      "\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:969)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1029)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:760)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:285)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "19/01/03 19:39:00 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 19,5,main]\n",
      "java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
      "\tat java.lang.Double.valueOf(Double.java:519)\n",
      "\tat scala.runtime.BoxesRunTime.boxToDouble(BoxesRunTime.java:79)\n",
      "\tat magellan.index.ZOrderCurve.<init>(ZOrderCurve.scala:28)\n",
      "\tat magellan.index.ZOrderCurveIndexer.index(ZOrderCurveIndexer.scala:63)\n",
      "\tat magellan.index.ZOrderCurveIndexer.cover(ZOrderCurveIndexer.scala:112)\n",
      "\tat magellan.index.ZOrderCurveIndexer.indexWithMeta(ZOrderCurveIndexer.scala:74)\n",
      "\tat org.apache.spark.sql.types.Indexer.nullSafeEval(Indexer.scala:43)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.UnaryExpression.eval(Expression.scala:331)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.Inline.eval(generators.scala:374)\n",
      "\tat org.apache.spark.sql.execution.GenerateExec$$anonfun$1$$anonfun$2.apply(GenerateExec.scala:93)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tat org.apache.spark.sql.execution.GenerateExec$$anonfun$1$$anonfun$2.apply(GenerateExec.scala:90)\n",
      "\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)\n",
      "\tat scala.collection.Iterator$JoinIterator.hasNext(Iterator.scala:212)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:395)\n",
      "\tat org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$1$$anon$1.hasNext(InMemoryRelation.scala:133)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:215)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1038)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1029)\n",
      "\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:969)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1029)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:760)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:285)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "19/01/03 19:39:00 WARN TaskSetManager: Lost task 0.0 in stage 16.0 (TID 19, localhost, executor driver): java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
      "\tat java.lang.Double.valueOf(Double.java:519)\n",
      "\tat scala.runtime.BoxesRunTime.boxToDouble(BoxesRunTime.java:79)\n",
      "\tat magellan.index.ZOrderCurve.<init>(ZOrderCurve.scala:28)\n",
      "\tat magellan.index.ZOrderCurveIndexer.index(ZOrderCurveIndexer.scala:63)\n",
      "\tat magellan.index.ZOrderCurveIndexer.cover(ZOrderCurveIndexer.scala:112)\n",
      "\tat magellan.index.ZOrderCurveIndexer.indexWithMeta(ZOrderCurveIndexer.scala:74)\n",
      "\tat org.apache.spark.sql.types.Indexer.nullSafeEval(Indexer.scala:43)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.UnaryExpression.eval(Expression.scala:331)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.Inline.eval(generators.scala:374)\n",
      "\tat org.apache.spark.sql.execution.GenerateExec$$anonfun$1$$anonfun$2.apply(GenerateExec.scala:93)\n",
      "\tat org.apache.spark.sql.execution.GenerateExec$$anonfun$1$$anonfun$2.apply(GenerateExec.scala:90)\n",
      "\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)\n",
      "\tat scala.collection.Iterator$JoinIterator.hasNext(Iterator.scala:212)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:395)\n",
      "\tat org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$1$$anon$1.hasNext(InMemoryRelation.scala:133)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:215)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1038)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1029)\n",
      "\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:969)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1029)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:760)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:285)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\n",
      "19/01/03 19:39:00 ERROR TaskSetManager: Task 0 in stage 16.0 failed 1 times; aborting job\n",
      "19/01/03 19:39:00 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool \n",
      "19/01/03 19:39:00 INFO SparkContext: Invoking stop() from shutdown hook\n",
      "19/01/03 19:39:00 INFO TaskSchedulerImpl: Cancelling stage 16\n",
      "19/01/03 19:39:00 INFO DAGScheduler: ResultStage 16 (show at cmd13.sc:1) failed in 1462.225 s due to Job aborted due to stage failure: Task 0 in stage 16.0 failed 1 times, most recent failure: Lost task 0.0 in stage 16.0 (TID 19, localhost, executor driver): java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
      "\tat java.lang.Double.valueOf(Double.java:519)\n",
      "\tat scala.runtime.BoxesRunTime.boxToDouble(BoxesRunTime.java:79)\n",
      "\tat magellan.index.ZOrderCurve.<init>(ZOrderCurve.scala:28)\n",
      "\tat magellan.index.ZOrderCurveIndexer.index(ZOrderCurveIndexer.scala:63)\n",
      "\tat magellan.index.ZOrderCurveIndexer.cover(ZOrderCurveIndexer.scala:112)\n",
      "\tat magellan.index.ZOrderCurveIndexer.indexWithMeta(ZOrderCurveIndexer.scala:74)\n",
      "\tat org.apache.spark.sql.types.Indexer.nullSafeEval(Indexer.scala:43)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.UnaryExpression.eval(Expression.scala:331)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.Inline.eval(generators.scala:374)\n",
      "\tat org.apache.spark.sql.execution.GenerateExec$$anonfun$1$$anonfun$2.apply(GenerateExec.scala:93)\n",
      "\tat org.apache.spark.sql.execution.GenerateExec$$anonfun$1$$anonfun$2.apply(GenerateExec.scala:90)\n",
      "\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)\n",
      "\tat scala.collection.Iterator$JoinIterator.hasNext(Iterator.scala:212)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:395)\n",
      "\tat org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$1$$anon$1.hasNext(InMemoryRelation.scala:133)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:215)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1038)\n",
      "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1029)\n",
      "\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:969)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1029)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:760)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:285)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
      "\n",
      "Driver stacktrace:\n",
      "19/01/03 19:39:00 INFO DAGScheduler: Job 7 failed: show at cmd13.sc:1, took 1462.310992 s\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31morg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 16.0 failed 1 times, most recent failure: Lost task 0.0 in stage 16.0 (TID 19, localhost, executor driver): java.lang.OutOfMemoryError: GC overhead limit exceeded\n\tat java.lang.Double.valueOf(Double.java:519)\n\tat scala.runtime.BoxesRunTime.boxToDouble(BoxesRunTime.java:79)\n\tat magellan.index.ZOrderCurve.<init>(ZOrderCurve.scala:28)\n\tat magellan.index.ZOrderCurveIndexer.index(ZOrderCurveIndexer.scala:63)\n\tat magellan.index.ZOrderCurveIndexer.cover(ZOrderCurveIndexer.scala:112)\n\tat magellan.index.ZOrderCurveIndexer.indexWithMeta(ZOrderCurveIndexer.scala:74)\n\tat org.apache.spark.sql.types.Indexer.nullSafeEval(Indexer.scala:43)\n\tat org.apache.spark.sql.catalyst.expressions.UnaryExpression.eval(Expression.scala:331)\n\tat org.apache.spark.sql.catalyst.expressions.Inline.eval(generators.scala:374)\n\tat org.apache.spark.sql.execution.GenerateExec$$anonfun$1$$anonfun$2.apply(GenerateExec.scala:93)\n\tat org.apache.spark.sql.execution.GenerateExec$$anonfun$1$$anonfun$2.apply(GenerateExec.scala:90)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)\n\tat scala.collection.Iterator$JoinIterator.hasNext(Iterator.scala:212)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:395)\n\tat org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$1$$anon$1.hasNext(InMemoryRelation.scala:133)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:215)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1038)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1029)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:969)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1029)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:760)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:285)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\nDriver stacktrace:\u001b[39m\n  org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m1517\u001b[39m)\n  org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m1505\u001b[39m)\n  org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m1504\u001b[39m)\n  scala.collection.mutable.ResizableArray$class.foreach(\u001b[32mResizableArray.scala\u001b[39m:\u001b[32m59\u001b[39m)\n  scala.collection.mutable.ArrayBuffer.foreach(\u001b[32mArrayBuffer.scala\u001b[39m:\u001b[32m48\u001b[39m)\n  org.apache.spark.scheduler.DAGScheduler.abortStage(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m1504\u001b[39m)\n  org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m814\u001b[39m)\n  org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m814\u001b[39m)\n  scala.Option.foreach(\u001b[32mOption.scala\u001b[39m:\u001b[32m257\u001b[39m)\n  org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m814\u001b[39m)\n  org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m1732\u001b[39m)\n  org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m1687\u001b[39m)\n  org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m1676\u001b[39m)\n  org.apache.spark.util.EventLoop$$anon$1.run(\u001b[32mEventLoop.scala\u001b[39m:\u001b[32m48\u001b[39m)\n  org.apache.spark.scheduler.DAGScheduler.runJob(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m630\u001b[39m)\n  org.apache.spark.SparkContext.runJob(\u001b[32mSparkContext.scala\u001b[39m:\u001b[32m2029\u001b[39m)\n  org.apache.spark.SparkContext.runJob(\u001b[32mSparkContext.scala\u001b[39m:\u001b[32m2050\u001b[39m)\n  org.apache.spark.SparkContext.runJob(\u001b[32mSparkContext.scala\u001b[39m:\u001b[32m2069\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan.executeTake(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m336\u001b[39m)\n  org.apache.spark.sql.execution.CollectLimitExec.executeCollect(\u001b[32mlimit.scala\u001b[39m:\u001b[32m38\u001b[39m)\n  org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(\u001b[32mDataset.scala\u001b[39m:\u001b[32m2861\u001b[39m)\n  org.apache.spark.sql.Dataset$$anonfun$head$1.apply(\u001b[32mDataset.scala\u001b[39m:\u001b[32m2150\u001b[39m)\n  org.apache.spark.sql.Dataset$$anonfun$head$1.apply(\u001b[32mDataset.scala\u001b[39m:\u001b[32m2150\u001b[39m)\n  org.apache.spark.sql.Dataset$$anonfun$55.apply(\u001b[32mDataset.scala\u001b[39m:\u001b[32m2842\u001b[39m)\n  org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m65\u001b[39m)\n  org.apache.spark.sql.Dataset.withAction(\u001b[32mDataset.scala\u001b[39m:\u001b[32m2841\u001b[39m)\n  org.apache.spark.sql.Dataset.head(\u001b[32mDataset.scala\u001b[39m:\u001b[32m2150\u001b[39m)\n  org.apache.spark.sql.Dataset.take(\u001b[32mDataset.scala\u001b[39m:\u001b[32m2363\u001b[39m)\n  org.apache.spark.sql.Dataset.showString(\u001b[32mDataset.scala\u001b[39m:\u001b[32m241\u001b[39m)\n  org.apache.spark.sql.Dataset.show(\u001b[32mDataset.scala\u001b[39m:\u001b[32m637\u001b[39m)\n  org.apache.spark.sql.Dataset.show(\u001b[32mDataset.scala\u001b[39m:\u001b[32m596\u001b[39m)\n  ammonite.$sess.cmd13$Helper.<init>(\u001b[32mcmd13.sc\u001b[39m:\u001b[32m1\u001b[39m)\n  ammonite.$sess.cmd13$.<init>(\u001b[32mcmd13.sc\u001b[39m:\u001b[32m7\u001b[39m)\n  ammonite.$sess.cmd13$.<clinit>(\u001b[32mcmd13.sc\u001b[39m:\u001b[32m-1\u001b[39m)\n\u001b[31mjava.lang.OutOfMemoryError: GC overhead limit exceeded\u001b[39m\n  java.lang.Double.valueOf(\u001b[32mDouble.java\u001b[39m:\u001b[32m519\u001b[39m)\n  scala.runtime.BoxesRunTime.boxToDouble(\u001b[32mBoxesRunTime.java\u001b[39m:\u001b[32m79\u001b[39m)\n  magellan.index.ZOrderCurve.<init>(\u001b[32mZOrderCurve.scala\u001b[39m:\u001b[32m28\u001b[39m)\n  magellan.index.ZOrderCurveIndexer.index(\u001b[32mZOrderCurveIndexer.scala\u001b[39m:\u001b[32m63\u001b[39m)\n  magellan.index.ZOrderCurveIndexer.cover(\u001b[32mZOrderCurveIndexer.scala\u001b[39m:\u001b[32m112\u001b[39m)\n  magellan.index.ZOrderCurveIndexer.indexWithMeta(\u001b[32mZOrderCurveIndexer.scala\u001b[39m:\u001b[32m74\u001b[39m)\n  org.apache.spark.sql.types.Indexer.nullSafeEval(\u001b[32mIndexer.scala\u001b[39m:\u001b[32m43\u001b[39m)\n  org.apache.spark.sql.catalyst.expressions.UnaryExpression.eval(\u001b[32mExpression.scala\u001b[39m:\u001b[32m331\u001b[39m)\n  org.apache.spark.sql.catalyst.expressions.Inline.eval(\u001b[32mgenerators.scala\u001b[39m:\u001b[32m374\u001b[39m)\n  org.apache.spark.sql.execution.GenerateExec$$anonfun$1$$anonfun$2.apply(\u001b[32mGenerateExec.scala\u001b[39m:\u001b[32m93\u001b[39m)\n  org.apache.spark.sql.execution.GenerateExec$$anonfun$1$$anonfun$2.apply(\u001b[32mGenerateExec.scala\u001b[39m:\u001b[32m90\u001b[39m)\n  scala.collection.Iterator$$anon$12.nextCur(\u001b[32mIterator.scala\u001b[39m:\u001b[32m435\u001b[39m)\n  scala.collection.Iterator$$anon$12.hasNext(\u001b[32mIterator.scala\u001b[39m:\u001b[32m441\u001b[39m)\n  scala.collection.Iterator$JoinIterator.hasNext(\u001b[32mIterator.scala\u001b[39m:\u001b[32m212\u001b[39m)\n  scala.collection.Iterator$$anon$11.hasNext(\u001b[32mIterator.scala\u001b[39m:\u001b[32m409\u001b[39m)\n  org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(\u001b[32mUnknown Source\u001b[39m)\n  org.apache.spark.sql.execution.BufferedRowIterator.hasNext(\u001b[32mBufferedRowIterator.java\u001b[39m:\u001b[32m43\u001b[39m)\n  org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(\u001b[32mWholeStageCodegenExec.scala\u001b[39m:\u001b[32m395\u001b[39m)\n  org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$1$$anon$1.hasNext(\u001b[32mInMemoryRelation.scala\u001b[39m:\u001b[32m133\u001b[39m)\n  org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(\u001b[32mMemoryStore.scala\u001b[39m:\u001b[32m215\u001b[39m)\n  org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(\u001b[32mBlockManager.scala\u001b[39m:\u001b[32m1038\u001b[39m)\n  org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(\u001b[32mBlockManager.scala\u001b[39m:\u001b[32m1029\u001b[39m)\n  org.apache.spark.storage.BlockManager.doPut(\u001b[32mBlockManager.scala\u001b[39m:\u001b[32m969\u001b[39m)\n  org.apache.spark.storage.BlockManager.doPutIterator(\u001b[32mBlockManager.scala\u001b[39m:\u001b[32m1029\u001b[39m)\n  org.apache.spark.storage.BlockManager.getOrElseUpdate(\u001b[32mBlockManager.scala\u001b[39m:\u001b[32m760\u001b[39m)\n  org.apache.spark.rdd.RDD.getOrCompute(\u001b[32mRDD.scala\u001b[39m:\u001b[32m334\u001b[39m)\n  org.apache.spark.rdd.RDD.iterator(\u001b[32mRDD.scala\u001b[39m:\u001b[32m285\u001b[39m)\n  org.apache.spark.rdd.MapPartitionsRDD.compute(\u001b[32mMapPartitionsRDD.scala\u001b[39m:\u001b[32m38\u001b[39m)\n  org.apache.spark.rdd.RDD.computeOrReadCheckpoint(\u001b[32mRDD.scala\u001b[39m:\u001b[32m323\u001b[39m)\n  org.apache.spark.rdd.RDD.iterator(\u001b[32mRDD.scala\u001b[39m:\u001b[32m287\u001b[39m)\n  org.apache.spark.rdd.MapPartitionsRDD.compute(\u001b[32mMapPartitionsRDD.scala\u001b[39m:\u001b[32m38\u001b[39m)\n  org.apache.spark.rdd.RDD.computeOrReadCheckpoint(\u001b[32mRDD.scala\u001b[39m:\u001b[32m323\u001b[39m)"
     ]
    }
   ],
   "source": [
    "dfLastIdwithVenueGeo.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfWithLocalizedEventTime\n",
    "    .select(col(\"event.time\"), col(\"event_time_localized\"), col(\"event_timezone\"))\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate & visualize most distinguishable distributions of meetings in particular day_of_week_local by tag (Jensen–Shannon divergence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "// total distribution\n",
    "import  org.apache.spark.sql.functions{lit, count, udf, collect_list}\n",
    "import  org.apache.spark.sql.types.{StringType, DoubleType, MapType}\n",
    "\n",
    "val countByAllWindowSpec = Window.partitionBy(lit(1))\n",
    "\n",
    "val totalWeekdayDistribution = dfWithLocalizedEventTime\n",
    "    .withColumn(\"event_isoweekday\", dfWithLocalizedEventTime.event_time_localized[3])\n",
    "    .select(col(\"rsvp_id\"), col(\"event_isoweekday\"), count(col(\"rsvp_id\")).over(countByAllWindowSpec).alias(\"weekday_total_share\")) \\\n",
    "    .groupBy(col(\"event_isoweekday\"), col(\"weekday_total_share\"))\n",
    "    .count()\n",
    "    .orderBy(\"event_isoweekday\")\n",
    "    .withColumn(\"total_dist\", col(\"count\")/col(\"weekday_total_share\")\n",
    "    .groupBy()\n",
    "    .agg(collect_list(col(\"total_dist\")).alias(\"total_dist\"))\n",
    "\n",
    "totalWeekdayDistribution.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "totalWeekdayDistribution.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "// distribution by group topic\n",
    "from pyspark.sql.functions import explode, lower, coalesce, abs\n",
    "from pyspark.sql.types import Row\n",
    "\n",
    "val countByTopicWindowSpec = Window.partitionBy(\"group_topic\")\n",
    "\n",
    "topicWeekdayDistributionTmp = dfWithLocalizedEventTime\n",
    "    .withColumn(\"event_isoweekday\", dfWithLocalizedEventTime.event_time_localized[3])\n",
    "    .select(col(\"rsvp_id\"), col(\"event_isoweekday\"), explode(col(\"group.group_topics\")).alias(\"group_topic_map\"))\n",
    "    .withColumn(\"group_topic\", col(\"group_topic_map\").getItem(\"urlkey\"))\n",
    "    .withColumn(\"weekday_topic_share\", count(\"rsvp_id\").over(countByTopicWindowSpec))\n",
    "    .drop(\"group_topic_map\")\n",
    "    .groupBy(col(\"event_isoweekday\"), col(\"group_topic\"), col(\"weekday_topic_share\"))\n",
    "    .count()\n",
    "    .orderBy(col(\"group_topic\"), col(\"event_isoweekday\"))\n",
    "\n",
    "val topics = topicWeekdayDistributionTmp.select(col(\"group_topic\").alias(\"group_topic_tmp\")).distinct()\n",
    "val weekdays = sc.parallelize(list(range(7))).map(lambda x: Row(event_isoweekday_tmp=str(1 + int(x)))).toDF()\n",
    "val cross = weekdays.crossJoin(topics).withColumn(\"count_tmp\", lit(0))\n",
    "\n",
    "// ensure that every topic has entry for every weekday (even if no meetings took place on that weekday)\n",
    "val topicWeekdayDistribution = cross\n",
    "    .join(topicWeekdayDistributionTmp, (topicWeekdayDistributionTmp.event_isoweekday == cross.event_isoweekday_tmp) & (topicWeekdayDistributionTmp.group_topic == cross.group_topic_tmp), how='outer') \\\n",
    "    .withColumn(\"event_isoweekday\", col(\"event_isoweekday_tmp\"))\n",
    "    .withColumn(\"group_topic\", col(\"group_topic_tmp\"))\n",
    "    .withColumn(\"count\", coalesce(\"count\", \"count_tmp\"))\n",
    "    .withColumn(\"weekday_topic_share\", coalesce(\"weekday_topic_share\", lit(-1)))\n",
    "    .drop(\"event_isoweekday_tmp\", \"group_topic_tmp\", \"count_tmp\")\n",
    "    .orderBy(\"group_topic\", \"event_isoweekday\")\n",
    "    .withColumn(\"topic_dist\", abs(col(\"count\")/col(\"weekday_topic_share\")))\n",
    "    .groupBy(\"group_topic\")\n",
    "    .agg(collect_list(col(\"topic_dist\")).alias(\"topic_dist\"))\n",
    "\n",
    "topicWeekdayDistribution.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "// JS Divergence UDF\n",
    "\n",
    "from numpy import asarray, e\n",
    "from scipy import stats\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "@udf(DoubleType())\n",
    "def udf_jsd(p, q, base=e):\n",
    "    '''\n",
    "        Implementation of pairwise `jsd` based on  \n",
    "        https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence\n",
    "    '''\n",
    "    try:\n",
    "        // convert to np.array\n",
    "        p, q = asarray(p), asarray(q)\n",
    "        // normalize p, q to probabilities\n",
    "        p, q = p/p.sum(), q/q.sum()\n",
    "\n",
    "        m = 1./2*(p + q)\n",
    "\n",
    "        return float(stats.entropy(p,m, base=base)/2. +  stats.entropy(q, m, base=base)/2.)\n",
    "    except:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "// calculate Jensen-Shannon Divergence per topic & select 10 highest\n",
    "val jsDivergence = topicWeekdayDistribution\n",
    "    .crossJoin(totalWeekdayDistribution)\n",
    "    .withColumn(\"jsd\", udf_jsd(col(\"topic_dist\"), col(\"total_dist\")))\n",
    "    .sort(col(\"jsd\").desc())\n",
    "    .limit(10)\n",
    "\n",
    "jsDivergence.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_hist(data_list):\n",
    "    import plotly.graph_objs as go\n",
    "    from plotly.offline import init_notebook_mode, iplot\n",
    "    from math import ceil\n",
    "    from plotly import tools\n",
    "    \n",
    "    init_notebook_mode(connected=True)\n",
    "    \n",
    "    from json import loads\n",
    "    \n",
    "    no = len(data_list)\n",
    "    \n",
    "    cols = 3\n",
    "    rows = ceil(no/cols)\n",
    "    \n",
    "    fig = tools.make_subplots(rows=rows, cols=cols, subplot_titles=[loads(x).get('group_topic', '') for x in data_list])\n",
    "\n",
    "    fig['layout'].update(height=1600, width=900, title='Most characteristic weekday dist', showlegend=False)\n",
    "\n",
    "    i = 0\n",
    "    \n",
    "    rows = [x+1 for x in range(rows)]\n",
    "    cols = [x+1 for x in range(cols)]\n",
    "    \n",
    "    combos = [(i,j) for i in rows for j in cols]\n",
    "    \n",
    "    print(combos)\n",
    "    \n",
    "    for data in data_list:\n",
    "        combo = combos[i]\n",
    "        \n",
    "        cur_row = combo[0]\n",
    "        cur_col = combo[1]\n",
    "        \n",
    "        data = loads(data)\n",
    "\n",
    "        x = [x+1 for x in range(6)]\n",
    "        y = data.get('topic_dist', [0 for x in range(6)])\n",
    "\n",
    "        title = data.get('group_topic', 'na')\n",
    "\n",
    "        fig.append_trace(go.Bar(x=x,y=y), cur_row, cur_col)\n",
    "        \n",
    "        i += 1\n",
    "        \n",
    "    iplot(fig, filename='make-subplots-multiple-with-titles')\n",
    "    \n",
    "entries = jsDivergence.toJSON().take(10)\n",
    "\n",
    "plot_hist(entries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate 'New Years Resolutions Effect' to establish which tags gained most interest inbetween december/january"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "// all available topics (if occured in one month and not other)\n",
    "val t = dfLastId\n",
    "    .select(explode(col(\"group.group_topics\")).alias(\"topic\"))\n",
    "    .select(col(\"topic\").getItem(\"urlkey\").alias(\"topic\"))\n",
    "    .distinct()\n",
    "\n",
    "// select topics from january, 2019\n",
    "val m1 = dfWithLocalizedEventTime\n",
    "    .where((col(\"event_time_localized\")[0] == '2019') & (col(\"event_time_localized\")[1] == '1'))\n",
    "    .select(explode(\"group.group_topics\").alias(\"topic\"))\n",
    "    .withColumn(\"topic_m1\", col(\"topic\").getItem(\"urlkey\"))\n",
    "    .groupBy(\"topic_m1\")\n",
    "    .count()\n",
    "    .alias(\"m1\")\n",
    "\n",
    "// select topics from december, 2018\n",
    "val m2 = dfWithLocalizedEventTime\n",
    "    .where((col(\"event_time_localized\")[0] == '2018') & (col(\"event_time_localized\")[1] == '12'))\n",
    "    .select(explode(\"group.group_topics\").alias(\"topic\"))\n",
    "    .withColumn(\"topic_m2\", col(\"topic\").getItem(\"urlkey\"))\n",
    "    .groupBy(\"topic_m2\")\n",
    "    .count()\n",
    "    .alias(\"m2\")\n",
    "\n",
    "// calculate increase in interest per topic & select 10 highest\n",
    "val increase = t\n",
    "    .join(m2, t.topic == m2.topic_m2, how='full')\n",
    "    .join(m1, t.topic == m1.topic_m1, how='full')\n",
    "    .withColumn(\"m1\", coalesce(col(\"m1.count\"), lit(\"0\")))\n",
    "    .withColumn(\"m2\", coalesce(col(\"m2.count\"), lit(\"0\")))\n",
    "    .select(col(\"topic\"), col(\"m1\"), col(\"m2\"))\n",
    "    .withColumn(\"change\", ((col(\"m1\")+col(\"m2\"))/2)*(col(\"m1\")/col(\"m2\")))\n",
    "    .sort(col(\"change\").desc())\n",
    "\n",
    "increase.cache()\n",
    "\n",
    "increase.limit(20).show()\n",
    "\n",
    "// show topics that were absent in december, 2018 but appeared in january, 2019\n",
    "val new_topics = increase\n",
    "    .where((col(\"m1\") > 0) & (col(\"m2\") == 0))\n",
    "\n",
    "new_topics.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.11 (almond)",
   "language": "scala",
   "name": "almond_scala_2_11"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
