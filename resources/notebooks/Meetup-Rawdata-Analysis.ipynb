{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connect to Spark & Elasticsearch, gather raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tzwhere in /opt/conda/lib/python3.6/site-packages (3.0.3)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.6/site-packages (2018.7)\n",
      "Requirement already satisfied: findspark in /opt/conda/lib/python3.6/site-packages (1.3.0)\n",
      "Requirement already satisfied: plotly in /opt/conda/lib/python3.6/site-packages (3.4.2)\n",
      "Requirement already satisfied: shapely in /opt/conda/lib/python3.6/site-packages (from tzwhere) (1.6.4.post2)\n",
      "Requirement already satisfied: retrying>=1.3.3 in /opt/conda/lib/python3.6/site-packages (from plotly) (1.3.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from plotly) (2.20.1)\n",
      "Requirement already satisfied: decorator>=4.0.6 in /opt/conda/lib/python3.6/site-packages (from plotly) (4.3.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from plotly) (1.11.0)\n",
      "Requirement already satisfied: nbformat>=4.2 in /opt/conda/lib/python3.6/site-packages (from plotly) (4.4.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->plotly) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->plotly) (1.23)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->plotly) (2018.10.15)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->plotly) (2.7)\n",
      "Requirement already satisfied: ipython_genutils in /opt/conda/lib/python3.6/site-packages (from nbformat>=4.2->plotly) (0.2.0)\n",
      "Requirement already satisfied: traitlets>=4.1 in /opt/conda/lib/python3.6/site-packages (from nbformat>=4.2->plotly) (4.3.2)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /opt/conda/lib/python3.6/site-packages (from nbformat>=4.2->plotly) (3.0.0a3)\n",
      "Requirement already satisfied: jupyter_core in /opt/conda/lib/python3.6/site-packages (from nbformat>=4.2->plotly) (4.4.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.6/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2->plotly) (18.2.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /opt/conda/lib/python3.6/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2->plotly) (0.14.6)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install tzwhere pytz findspark plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "import os\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--jars jars/elasticsearch-spark-20_2.11-6.5.1.jar pyspark-shell'\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "sc = SparkContext(appName=\"esAnalytics\")\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"meetup\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .config(\"spark.sql.crossJoin.enabled\", \"true\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/python/pyspark/sql/session.py:366: UserWarning: Using RDD of dict to inferSchema is deprecated. Use pyspark.sql.Row instead\n",
      "  warnings.warn(\"Using RDD of dict to inferSchema is deprecated. \"\n"
     ]
    }
   ],
   "source": [
    "from json import loads, dumps\n",
    "\n",
    "es_read_conf = {\n",
    "\"es.nodes\" : 'elastic',\n",
    "\"es.port\" : '9200',\n",
    "\"es.resource\" : 'meetup-rawdata-*/default'\n",
    "}\n",
    "\n",
    "raw_data = sc.newAPIHadoopRDD(\n",
    "inputFormatClass=\"org.elasticsearch.hadoop.mr.EsInputFormat\",\n",
    "keyClass=\"org.apache.hadoop.io.NullWritable\",\n",
    "valueClass=\"org.elasticsearch.hadoop.mr.LinkedMapWritable\",\n",
    "conf=es_read_conf)\n",
    "\n",
    "raw_data = raw_data.map(lambda v: loads(dumps(v[1])))\n",
    "\n",
    "df = sqlContext.createDataFrame(raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select only last response for each rsvp_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, col\n",
    "\n",
    "rsvpWindowSpec = Window.partitionBy(df[\"rsvp_id\"]).orderBy(df[\"mtime\"].desc())\n",
    "\n",
    "df = df \\\n",
    "    .withColumn(\"rowId\", row_number().over(rsvpWindowSpec)) \\\n",
    "    .where(\"rowId = 1\") \\\n",
    "    .orderBy(\"rsvp_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Establish timezone, day_of_week_local, hour_local, minute_local of event.event_time based on venue.venue_geo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytz\n",
    "\n",
    "from datetime import datetime\n",
    "from tzwhere import tzwhere\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType, ArrayType\n",
    "\n",
    "@udf(StringType())\n",
    "def udf_timezone_by_geo(lat, lon):\n",
    "    t = tzwhere.tzwhere()\n",
    "    \n",
    "    return t.tzNameAt(float(lat), float(lon))\n",
    "\n",
    "@udf(ArrayType(StringType()))\n",
    "def udf_localize_with_timezone(utc_time, timezone_str):\n",
    "    \n",
    "    epoch_utc = int(utc_time)\n",
    "    \n",
    "    timezone_str = timezone_str.strip()\n",
    "    \n",
    "    # check if provided in ms or s:\n",
    "    if len(str(epoch_utc)) == 13:\n",
    "        epoch_utc = epoch_utc / 1000\n",
    "\n",
    "    # get time in UTC\n",
    "    utc_dt = datetime.utcfromtimestamp(epoch_utc)\n",
    "\n",
    "    # convert it to tz\n",
    "    tz = pytz.timezone(timezone_str)\n",
    "    dt = utc_dt.astimezone(tz)\n",
    "\n",
    "    offset = dt.utcoffset().total_seconds()\n",
    "\n",
    "    local_dt = datetime.utcfromtimestamp(epoch_utc + offset)\n",
    "    \n",
    "    parts = dict(year_local=local_dt.year,\n",
    "                 month_local=local_dt.month, \n",
    "                 day_local=local_dt.day, \n",
    "                 weekday_local=local_dt.isoweekday(),\n",
    "                 hour_local=local_dt.hour, \n",
    "                 minute_local=local_dt.minute)\n",
    "    \n",
    "    return list(parts.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------+\n",
      "|              key|event_timezone|\n",
      "+-----------------+--------------+\n",
      "|52.22977_21.01178| Europe/Warsaw|\n",
      "+-----------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# to optimize matching event.time with venue.lat/venue.lon create dict with distinct venues\n",
    "\n",
    "from pyspark.sql.functions import concat, lit\n",
    "\n",
    "venueGeoDict = df \\\n",
    "    .select(col(\"venue.lat\"), col(\"venue.lon\")) \\\n",
    "    .distinct() \\\n",
    "    .withColumn(\"key\", concat(col(\"lat\"), lit(\"_\"), col(\"lon\"))) \\\n",
    "    .withColumn(\"event_timezone\", udf_timezone_by_geo(col(\"lat\"), col(\"lon\"))) \\\n",
    "    .select(col(\"key\"), col(\"event_timezone\"))\n",
    "\n",
    "venueGeoDict.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[event: map<string,string>, group: map<string,array<map<string,string>>>, guests: double, member: map<string,double>, mtime: bigint, response: string, rsvp_id: double, venue: map<string,string>, visibility: string, rowId: int, key: string, event_timezone: string, event_time_localized: array<string>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfWithEventTimezone = df \\\n",
    "    .join(venueGeoDict, concat(col(\"venue.lat\"), lit(\"_\"), col(\"venue.lon\")) == venueGeoDict.key, 'cross') \n",
    "\n",
    "dfWithLocalizedEventTime = dfWithEventTimezone \\\n",
    "    .withColumn(\"event_time_localized\", udf_localize_with_timezone(dfWithEventTimezone.event.time, dfWithEventTimezone.event_timezone))\n",
    "\n",
    "dfWithLocalizedEventTime.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+--------------+\n",
      "|         time|event_time_localized|event_timezone|\n",
      "+-------------+--------------------+--------------+\n",
      "|1544217874000|[2018, 12, 7, 5, ...| Europe/Warsaw|\n",
      "|1544239170000|[2018, 12, 8, 6, ...| Europe/Warsaw|\n",
      "|1544257659000|[2018, 12, 8, 6, ...| Europe/Warsaw|\n",
      "|1544242550000|[2018, 12, 8, 6, ...| Europe/Warsaw|\n",
      "|1544235224000|[2018, 12, 8, 6, ...| Europe/Warsaw|\n",
      "+-------------+--------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfWithLocalizedEventTime \\\n",
    "    .select(col(\"event.time\"), col(\"event_time_localized\"), col(\"event_timezone\")) \\\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate & visualize most distinguishable distributions of meetings in particular day_of_week_local by tag (Jensenâ€“Shannon divergence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[total_dist: array<double>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total distribution\n",
    "\n",
    "from pyspark.sql.functions import lit, count, udf, collect_list\n",
    "from pyspark.sql.types import StringType, DoubleType, MapType\n",
    "\n",
    "countByAllWindowSpec = Window.partitionBy(lit(1))\n",
    "\n",
    "totalWeekdayDistribution = dfWithLocalizedEventTime \\\n",
    "    .withColumn(\"event_isoweekday\", dfWithLocalizedEventTime.event_time_localized[3]) \\\n",
    "    .select(col(\"rsvp_id\"), col(\"event_isoweekday\"), count(col(\"rsvp_id\")).over(countByAllWindowSpec).alias(\"weekday_total_share\")) \\\n",
    "    .groupBy(col(\"event_isoweekday\"), col(\"weekday_total_share\")) \\\n",
    "    .count() \\\n",
    "    .orderBy(\"event_isoweekday\") \\\n",
    "    .withColumn(\"total_dist\", col(\"count\")/col(\"weekday_total_share\")) \\\n",
    "    .groupBy() \\\n",
    "    .agg(collect_list(col(\"total_dist\")).alias(\"total_dist\"))\n",
    "\n",
    "totalWeekdayDistribution.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(total_dist=[0.11764705882352941, 0.09411764705882353, 0.1264705882352941, 0.11666666666666667, 0.10490196078431373, 0.21176470588235294, 0.2284313725490196])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalWeekdayDistribution.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[group_topic: string, topic_dist: array<double>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# distribution by group topic\n",
    "from pyspark.sql.functions import explode, lower, coalesce, abs\n",
    "from pyspark.sql.types import Row\n",
    "\n",
    "countByTopicWindowSpec = Window.partitionBy(\"group_topic\")\n",
    "\n",
    "topicWeekdayDistributionTmp = dfWithLocalizedEventTime \\\n",
    "    .withColumn(\"event_isoweekday\", dfWithLocalizedEventTime.event_time_localized[3]) \\\n",
    "    .select(col(\"rsvp_id\"), col(\"event_isoweekday\"), explode(col(\"group.group_topics\")).alias(\"group_topic_map\")) \\\n",
    "    .withColumn(\"group_topic\", col(\"group_topic_map\").getItem(\"urlkey\")) \\\n",
    "    .withColumn(\"weekday_topic_share\", count(\"rsvp_id\").over(countByTopicWindowSpec)) \\\n",
    "    .drop(\"group_topic_map\") \\\n",
    "    .groupBy(col(\"event_isoweekday\"), col(\"group_topic\"), col(\"weekday_topic_share\")) \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"group_topic\"), col(\"event_isoweekday\"))\n",
    "\n",
    "topics = topicWeekdayDistributionTmp.select(col(\"group_topic\").alias(\"group_topic_tmp\")).distinct()\n",
    "weekdays = sc.parallelize(list(range(7))).map(lambda x: Row(event_isoweekday_tmp=str(1 + int(x)))).toDF()\n",
    "cross = weekdays.crossJoin(topics).withColumn(\"count_tmp\", lit(0))\n",
    "\n",
    "# ensure that every topic has entry for every weekday (even if no meetings took place on that weekday)\n",
    "topicWeekdayDistribution = cross \\\n",
    "    .join(topicWeekdayDistributionTmp, (topicWeekdayDistributionTmp.event_isoweekday == cross.event_isoweekday_tmp) & (topicWeekdayDistributionTmp.group_topic == cross.group_topic_tmp), how='outer') \\\n",
    "    .withColumn(\"event_isoweekday\", col(\"event_isoweekday_tmp\")) \\\n",
    "    .withColumn(\"group_topic\", col(\"group_topic_tmp\")) \\\n",
    "    .withColumn(\"count\", coalesce(\"count\", \"count_tmp\")) \\\n",
    "    .withColumn(\"weekday_topic_share\", coalesce(\"weekday_topic_share\", lit(-1))) \\\n",
    "    .drop(\"event_isoweekday_tmp\", \"group_topic_tmp\", \"count_tmp\") \\\n",
    "    .orderBy(\"group_topic\", \"event_isoweekday\") \\\n",
    "    .withColumn(\"topic_dist\", abs(col(\"count\")/col(\"weekday_topic_share\"))) \\\n",
    "    .groupBy(\"group_topic\") \\\n",
    "    .agg(collect_list(col(\"topic_dist\")).alias(\"topic_dist\"))\n",
    "\n",
    "topicWeekdayDistribution.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JS Divergence UDF\n",
    "\n",
    "from numpy import asarray, e\n",
    "from scipy import stats\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "@udf(DoubleType())\n",
    "def udf_jsd(p, q, base=e):\n",
    "    '''\n",
    "        Implementation of pairwise `jsd` based on  \n",
    "        https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence\n",
    "    '''\n",
    "    try:\n",
    "        ## convert to np.array\n",
    "        p, q = asarray(p), asarray(q)\n",
    "        ## normalize p, q to probabilities\n",
    "        p, q = p/p.sum(), q/q.sum()\n",
    "\n",
    "        m = 1./2*(p + q)\n",
    "\n",
    "        return float(stats.entropy(p,m, base=base)/2. +  stats.entropy(q, m, base=base)/2.)\n",
    "    except:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[group_topic: string, topic_dist: array<double>, total_dist: array<double>, jsd: double]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate Jensen-Shannon Divergence per topic & select 10 highest\n",
    "jsDivergence = topicWeekdayDistribution \\\n",
    "    .crossJoin(totalWeekdayDistribution) \\\n",
    "    .withColumn(\"jsd\", udf_jsd(col(\"topic_dist\"), col(\"total_dist\"))) \\\n",
    "    .sort(col(\"jsd\").desc()) \\\n",
    "    .limit(10)\n",
    "\n",
    "jsDivergence.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist(data_list):\n",
    "    import plotly.graph_objs as go\n",
    "    from plotly.offline import init_notebook_mode, iplot\n",
    "    from math import ceil\n",
    "    from plotly import tools\n",
    "    \n",
    "    init_notebook_mode(connected=True)\n",
    "    \n",
    "    from json import loads\n",
    "    \n",
    "    no = len(data_list)\n",
    "    \n",
    "    cols = 3\n",
    "    rows = ceil(no/cols)\n",
    "    \n",
    "    fig = tools.make_subplots(rows=rows, cols=cols, subplot_titles=[loads(x).get('group_topic', '') for x in data_list])\n",
    "\n",
    "    fig['layout'].update(height=1600, width=900, title='Most characteristic weekday dist', showlegend=False)\n",
    "\n",
    "    i = 0\n",
    "    \n",
    "    rows = [x+1 for x in range(rows)]\n",
    "    cols = [x+1 for x in range(cols)]\n",
    "    \n",
    "    combos = [(i,j) for i in rows for j in cols]\n",
    "    \n",
    "    print(combos)\n",
    "    \n",
    "    for data in data_list:\n",
    "        combo = combos[i]\n",
    "        \n",
    "        cur_row = combo[0]\n",
    "        cur_col = combo[1]\n",
    "        \n",
    "        data = loads(data)\n",
    "\n",
    "        x = [x+1 for x in range(6)]\n",
    "        y = data.get('topic_dist', [0 for x in range(6)])\n",
    "\n",
    "        title = data.get('group_topic', 'na')\n",
    "\n",
    "        fig.append_trace(go.Bar(x=x,y=y), cur_row, cur_col)\n",
    "        \n",
    "        i += 1\n",
    "        \n",
    "    iplot(fig, filename='make-subplots-multiple-with-titles')\n",
    "    \n",
    "entries = jsDivergence.toJSON().take(10)\n",
    "\n",
    "plot_hist(entries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate 'New Years Resolutions Effect' to establish which tags gained most interest inbetween december/january"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all available topics (if occured in one month and not other)\n",
    "t = df \\\n",
    "    .select(explode(col(\"group.group_topics\")).alias(\"topic\")) \\\n",
    "    .select(col(\"topic\").getItem(\"urlkey\").alias(\"topic\")) \\\n",
    "    .distinct()\n",
    "\n",
    "# select topics from january, 2019\n",
    "m1 = dfWithLocalizedEventTime \\\n",
    "    .where((col(\"event_time_localized\")[0] == '2019') & (col(\"event_time_localized\")[1] == '1')) \\\n",
    "    .select(explode(\"group.group_topics\").alias(\"topic\")) \\\n",
    "    .withColumn(\"topic_m1\", col(\"topic\").getItem(\"urlkey\")) \\\n",
    "    .groupBy(\"topic_m1\") \\\n",
    "    .count() \\\n",
    "    .alias(\"m1\")\n",
    "\n",
    "# select topics from december, 2018\n",
    "m2 = dfWithLocalizedEventTime \\\n",
    "    .where((col(\"event_time_localized\")[0] == '2018') & (col(\"event_time_localized\")[1] == '12')) \\\n",
    "    .select(explode(\"group.group_topics\").alias(\"topic\")) \\\n",
    "    .withColumn(\"topic_m2\", col(\"topic\").getItem(\"urlkey\")) \\\n",
    "    .groupBy(\"topic_m2\") \\\n",
    "    .count() \\\n",
    "    .alias(\"m2\")\n",
    "\n",
    "# calculate increase in interest per topic & select 10 highest\n",
    "increase = t \\\n",
    "    .join(m2, t.topic == m2.topic_m2, how='full') \\\n",
    "    .join(m1, t.topic == m1.topic_m1, how='full') \\\n",
    "    .withColumn(\"m1\", coalesce(col(\"m1.count\"), lit(\"0\"))) \\\n",
    "    .withColumn(\"m2\", coalesce(col(\"m2.count\"), lit(\"0\"))) \\\n",
    "    .select(col(\"topic\"), col(\"m1\"), col(\"m2\")) \\\n",
    "    .withColumn(\"change\", ((col(\"m1\")+col(\"m2\"))/2)*(col(\"m1\")/col(\"m2\"))) \\\n",
    "    .sort(col(\"change\").desc()) \\\n",
    "    .limit(20)\n",
    "\n",
    "increase.show()\n",
    "\n",
    "# show topics that were absent in december, 2018 but appeared in january, 2019\n",
    "new_topics = increase \\\n",
    "    .where((col(\"m1\") > 0) & (col(\"m2\") == 0))\n",
    "\n",
    "new_topics.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
