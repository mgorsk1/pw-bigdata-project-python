{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connect to Spark & Elasticsearch, gather raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "import os\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--jars jars/elasticsearch-spark-20_2.11-6.5.1.jar pyspark-shell'\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "sc = SparkContext(appName=\"esTest\")\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"meetup\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import loads, dumps\n",
    "\n",
    "es_read_conf = {\n",
    "\"es.nodes\" : 'elastic',\n",
    "\"es.port\" : '9200',\n",
    "\"es.resource\" : 'meetup-rawdata-*/default'\n",
    "}\n",
    "\n",
    "raw_data = sc.newAPIHadoopRDD(\n",
    "inputFormatClass=\"org.elasticsearch.hadoop.mr.EsInputFormat\",\n",
    "keyClass=\"org.apache.hadoop.io.NullWritable\",\n",
    "valueClass=\"org.elasticsearch.hadoop.mr.LinkedMapWritable\",\n",
    "conf=es_read_conf)\n",
    "\n",
    "raw_data = raw_data.map(lambda v: loads(dumps(v[1])))\n",
    "\n",
    "df = sqlContext.createDataFrame(raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select only last response for each rsvp_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "rsvpWindowSpec = Window.partitionBy(df[\"rsvp_id\"]).orderBy(df[\"mtime\"].desc())\n",
    "\n",
    "df = df\\\n",
    "    .withColumn(\"rowId\", row_number().over(rsvpWindowSpec))\\\n",
    "    .where(\"rowId = 1\")\\\n",
    "    .orderBy(\"rsvp_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Establish timezone, day_of_week_local, hour_local, minute_local of event.event_time based on venue.venue_geo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install tzwhere pytz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytz\n",
    "\n",
    "from datetime import datetime\n",
    "from tzwhere import tzwhere\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType, StringType\n",
    "\n",
    "def utc_epoch_to_local_by_coords(epoch_utc, lat, lon):\n",
    "    t = tzwhere.tzwhere()\n",
    "    \n",
    "    lat = float(lat)\n",
    "    lon = float(lon)\n",
    "    \n",
    "    epoch_utc = int(epoch_utc)\n",
    "    \n",
    "    # check if provided in ms or s:\n",
    "    if len(str(epoch_utc)) == 13:\n",
    "        epoch_utc = epoch_utc / 1000\n",
    "\n",
    "    timezone_str = t.tzNameAt(lat, lon)\n",
    "\n",
    "    # get time in UTC\n",
    "    utc_dt = datetime.utcfromtimestamp(epoch_utc)\n",
    "\n",
    "    # convert it to tz\n",
    "    tz = pytz.timezone(timezone_str)\n",
    "    dt = utc_dt.astimezone(tz)\n",
    "\n",
    "    offset = dt.utcoffset().total_seconds()\n",
    "\n",
    "    local_dt = datetime.utcfromtimestamp(epoch_utc + offset)\n",
    "\n",
    "    return \"_\".join([str(x) for x in dict(month_local=local_dt.month, day_local=local_dt.day, weekday_local=local_dt.weekday(),\n",
    "                     hour_local=local_dt.hour, minute_local=local_dt.minute).values()])\n",
    "\n",
    "udf_utc_epoch_to_local_by_coords = udf(utc_epoch_to_local_by_coords, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df \\\n",
    ".withColumn(\"local\", udf_utc_epoch_to_local_by_coords(df.event.time, df.venue.lat, df.venue.lon)) \\\n",
    ".collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate & visualize most distinguishable distributions of meetings in particular day_of_week_local by tag (Jensenâ€“Shannon divergence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate 'New Years Resolutions Effect' to establish which tags gained most interest inbetween december/january"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
